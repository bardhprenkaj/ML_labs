{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5AYt3Cn35zA"
      },
      "source": [
        "**Challenge: Implement a Multiclass Classification Neural Network using PyTorch**\n",
        "\n",
        "Objective:\n",
        "Build a feedforward neural network using PyTorch to predict the species of iris flowers in a multiclass classification problem. The dataset used for this challenge is the Iris dataset, which consists of features like sepal length, sepal width, petal length, and petal width.\n",
        "\n",
        "Steps:\n",
        "\n",
        "1. **Data Preparation**: Load the MNIST dataset using ```torchvision.datasets.MNIST```. Standardize/normalize the features. Split the dataset into training and testing sets using, for example, ```sklearn.model_selection.train_test_split()```. **Bonus scores**: *use PyTorch's built-* ```DataLoader``` *to split the dataset*.\n",
        "\n",
        "2. **Neural Network Architecture**: Define a simple feedforward neural network using PyTorch's ```nn.Module```. Design the input layer to match the number of features in the MNIST dataset and the output layer to have as many neurons as there are classes (10). You can experiment with the number of hidden layers and neurons to optimize the performance. **Bonus scores**: *Make your architecture flexibile to have as many hidden layers as the user wants, and use hyperparameter optimization to select the best number of hidden layeres.*\n",
        "\n",
        "3. **Loss Function and Optimizer**: Choose an appropriate loss function for multiclass classification. Select an optimizer, like SGD (Stochastic Gradient Descent) or Adam.\n",
        "\n",
        "4. **Training**: Write a training loop to iterate over the dataset.\n",
        "Forward pass the input through the network, calculate the loss, and perform backpropagation. Update the weights of the network using the chosen optimizer.\n",
        "\n",
        "5. **Testing**: Evaluate the trained model on the test set. Calculate the accuracy of the model.\n",
        "\n",
        "6. **Optimization**: Experiment with hyperparameters (learning rate, number of epochs, etc.) to optimize the model's performance. Consider adjusting the neural network architecture for better results. **Notice that you can't use the optimization algorithms from scikit-learn that we saw in lab1: e.g.,** ```GridSearchCV```.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "5XLynrxJ33v6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1) Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define a transform to normalize the data\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "batch_size = 16\n",
        "\n",
        "train_loader = DataLoader(\n",
        "  torchvision.datasets.MNIST(root='./data', train=True, download=True,transform=transform),\n",
        "  batch_size=batch_size, shuffle=True)\n",
        "\n",
        "test_loader = DataLoader(\n",
        "  torchvision.datasets.MNIST(root='./data', train=False, download=True,\n",
        "                             transform=transform),\n",
        "  batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2) Neural Network Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MyNN(nn.Module):\n",
        "    def __init__(self,nb_hidden_layers):\n",
        "        super(MyNN,self).__init__()\n",
        "        self.fc1 = nn.Linear(28*28, 512)\n",
        "        self.hidden_list = nn.ModuleList([nn.Linear(512, 512) for _ in range (nb_hidden_layers)])\n",
        "        self.fc3 = nn.Linear(512, 10)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "\n",
        "    def forward(self, img): #convert + flatten\n",
        "        x = img.view(-1, 28*28)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        for layer in self.hidden_list: \n",
        "            x = self.relu(layer(x))\n",
        "            x = self.dropout(x)\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "    \n",
        "model = MyNN(nb_hidden_layers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "MyNN(\n",
              "  (fc1): Linear(in_features=784, out_features=512, bias=True)\n",
              "  (hidden_list): ModuleList(\n",
              "    (0-1): 2 x Linear(in_features=512, out_features=512, bias=True)\n",
              "  )\n",
              "  (fc3): Linear(in_features=512, out_features=10, bias=True)\n",
              "  (relu): ReLU()\n",
              "  (dropout): Dropout(p=0.2, inplace=False)\n",
              ")"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3) Loss Function and Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4) Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Finished epoch 0, latest loss 0.35597026348114014\n",
            "Finished epoch 1, latest loss 0.08412156999111176\n",
            "Finished epoch 2, latest loss 0.3455660939216614\n",
            "Finished epoch 3, latest loss 0.0978328287601471\n",
            "Finished epoch 4, latest loss 0.018433762714266777\n",
            "Finished epoch 5, latest loss 0.13362887501716614\n",
            "Finished epoch 6, latest loss 0.07205747067928314\n",
            "Finished epoch 7, latest loss 0.016014179214835167\n",
            "Finished epoch 8, latest loss 0.03405395522713661\n",
            "Finished epoch 9, latest loss 0.12341000884771347\n",
            "Finished epoch 10, latest loss 0.003646095981821418\n",
            "Finished epoch 11, latest loss 0.06788351386785507\n",
            "Finished epoch 12, latest loss 0.004085300024598837\n",
            "Finished epoch 13, latest loss 0.0982888713479042\n",
            "Finished epoch 14, latest loss 0.1406562626361847\n",
            "Finished epoch 15, latest loss 0.012005725875496864\n",
            "Finished epoch 16, latest loss 0.0329718217253685\n",
            "Finished epoch 17, latest loss 0.0033584078773856163\n",
            "Finished epoch 18, latest loss 0.0011928463354706764\n",
            "Finished epoch 19, latest loss 0.006740660406649113\n"
          ]
        }
      ],
      "source": [
        "n_epochs = 20\n",
        "\n",
        "model.train()\n",
        "\n",
        "# Training loop over a specified number of epochs\n",
        "for epoch in range(n_epochs):\n",
        "    \n",
        "    for data,target in train_loader:\n",
        "        \n",
        "        # Zero the gradients accumulated in previous iterations\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        output = model(data)\n",
        "\n",
        "        # Calculate the loss\n",
        "        loss = criterion(output, target)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # Update the model parameters\n",
        "        optimizer.step()\n",
        "\n",
        "    # Print information about the current epoch\n",
        "    print(f'Finished epoch {epoch}, latest loss {loss}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5) Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy of 0: 99% (970/980)\n",
            "Test Accuracy of 1: 99% (1125/1135)\n",
            "Test Accuracy of 2: 97% (1003/1032)\n",
            "Test Accuracy of 3: 98% (992/1010)\n",
            "Test Accuracy of 4: 99% (971/982)\n",
            "Test Accuracy of 5: 97% (865/892)\n",
            "Test Accuracy of 6: 98% (937/958)\n",
            "Test Accuracy of 7: 98% (1005/1028)\n",
            "Test Accuracy of 8: 96% (934/974)\n",
            "Test Accuracy of 9: 97% (976/1009)\n",
            "Test Accuracy Overall: 98% (9778/10000)\n"
          ]
        }
      ],
      "source": [
        "class_correct = [0 for i in range(10)] \n",
        "class_total = [0 for i in range(10)]\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "    for data, target in test_loader:\n",
        "        \n",
        "        # Forward pass\n",
        "        output = model(data)\n",
        "        \n",
        "        # Calculate the loss\n",
        "        loss = criterion(output, target)\n",
        "        \n",
        "        # Convert output probabilities to predicted class\n",
        "        _, pred = torch.max(output, 1)\n",
        "        \n",
        "        # Compare predictions to true label\n",
        "        correct = np.squeeze(pred.eq(target.data.view_as(pred)))\n",
        "        \n",
        "        # Calculate test accuracy for each object class\n",
        "        for i in range(batch_size):\n",
        "            label = target.data[i]\n",
        "            class_correct[label] += correct[i].item()\n",
        "            class_total[label] += 1\n",
        "\n",
        "for i in range(10):\n",
        "    print(f'Test Accuracy of {i}: {round(100 * class_correct[i] / class_total[i])}% ({np.sum(class_correct[i])}/{np.sum(class_total[i])})') \n",
        "    \n",
        "print(f'Test Accuracy Overall: {round(100 * np.sum(class_correct) / np.sum(class_total))}% ({np.sum(class_correct)}/{np.sum(class_total)})') "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6) Optimization\n",
        "\n",
        "We have already a good accuracy on this model, I got those hyperparameters by test and retry, and it takes too much effort to do optimization only to get 2-3% (It's also long to train the model)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
