{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Challenge: Implement a Multiclass Classification Neural Network using PyTorch**\n",
        "\n",
        "Objective:\n",
        "Build a neural network using PyTorch to predict handwritten digits of MNIST.\n",
        "\n",
        "Steps:\n",
        "\n",
        "1. **Data Preparation**: Load the MNIST dataset using ```torchvision.datasets.MNIST```. Standardize/normalize the features. Split the dataset into training and testing sets using, for example, ```sklearn.model_selection.train_test_split()```. **Bonus scores**: *use PyTorch's built-* ```DataLoader``` *to split the dataset*.\n",
        "\n",
        "2. **Neural Network Architecture**: Define a simple feedforward neural network using PyTorch's ```nn.Module```. Design the input layer to match the number of features in the MNIST dataset and the output layer to have as many neurons as there are classes (10). You can experiment with the number of hidden layers and neurons to optimize the performance. **Bonus scores**: *Make your architecture flexibile to have as many hidden layers as the user wants, and use hyperparameter optimization to select the best number of hidden layeres.*\n",
        "\n",
        "3. **Loss Function and Optimizer**: Choose an appropriate loss function for multiclass classification. Select an optimizer, like SGD (Stochastic Gradient Descent) or Adam.\n",
        "\n",
        "4. **Training**: Write a training loop to iterate over the dataset.\n",
        "Forward pass the input through the network, calculate the loss, and perform backpropagation. Update the weights of the network using the chosen optimizer.\n",
        "\n",
        "5. **Testing**: Evaluate the trained model on the test set. Calculate the accuracy of the model.\n",
        "\n",
        "6. **Optimization**: Experiment with hyperparameters (learning rate, number of epochs, etc.) to optimize the model's performance. Consider adjusting the neural network architecture for better results. **Notice that you can't use the optimization algorithms from scikit-learn that we saw in lab1: e.g.,** ```GridSearchCV```.\n"
      ],
      "metadata": {
        "id": "E5AYt3Cn35zA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5XLynrxJ33v6"
      },
      "outputs": [],
      "source": [
        "#imports\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#1\n",
        "\n",
        "# we normalize the images in the range [-1,1]\n",
        "transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize(0.5, 0.5)])\n",
        "\n",
        "# get the training and test set of MNIST\n",
        "trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "testset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# get the training and test data\n",
        "batch_size = 32\n",
        "trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
        "testloader = DataLoader(testset, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "0BhGDBMlOari"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2\n",
        "\n",
        "# In this case all hidden layers have the same size. To make the dimension of every hidden layer customizable\n",
        "# by the user we could insert an array as a parameter containg all the dimensions and modify the for cycle\n",
        "# making each input of each layer have the same dimension of the output of the layer before.\n",
        "\n",
        "class NET(nn.Module):\n",
        "  def __init__(self, input_size, hidden_layers, hidden_size):\n",
        "    super(NET, self).__init__()\n",
        "\n",
        "    # Define input layer\n",
        "    self.input_layer = nn.Linear(input_size, hidden_size)\n",
        "\n",
        "    # Define hidden layers\n",
        "    self.hidden_layers = nn.ModuleList([\n",
        "      nn.Linear(hidden_size, hidden_size) for _ in range(hidden_layers)\n",
        "    ])\n",
        "\n",
        "    # Define output layer\n",
        "    self.output_layer = nn.Linear(hidden_size, 10)\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    x = x.view(x.size(0), -1)\n",
        "\n",
        "    # Apply activation function (e.g., ReLU) to input layer\n",
        "    x = torch.relu(self.input_layer(x))\n",
        "\n",
        "    # Apply activation function and pass through each hidden layer\n",
        "    for layer in self.hidden_layers:\n",
        "      x = torch.relu(layer(x))\n",
        "\n",
        "    # Output layer without activation\n",
        "    x = self.output_layer(x)\n",
        "\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "a6VW3GJ-T0V0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = 28*28\n",
        "hidden_layers = 2\n",
        "hidden_size = 256\n",
        "\n",
        "model = NET(input_size, hidden_layers, hidden_size)"
      ],
      "metadata": {
        "id": "xYbAWKafeDbg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#3\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
      ],
      "metadata": {
        "id": "nTKRJpnzL9n6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#4\n",
        "\n",
        "n_epochs = 5\n",
        "\n",
        "# Training\n",
        "for epoch in range(n_epochs):\n",
        "\n",
        "  loss = 0\n",
        "\n",
        "  for x,y in trainloader:\n",
        "\n",
        "    ypred = model(x)\n",
        "    loss = loss_fn(ypred, y)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  print(f'Finished epoch {epoch + 1}, latest loss {loss}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dt38bvXXq9sC",
        "outputId": "df8d7d03-cbda-40b8-89cc-b6ceab5f3c3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished epoch 1, latest loss 0.00017945421859622002\n",
            "Finished epoch 2, latest loss 0.001070813974365592\n",
            "Finished epoch 3, latest loss 0.07683932036161423\n",
            "Finished epoch 4, latest loss 0.010269719175994396\n",
            "Finished epoch 5, latest loss 0.00015121296746656299\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#5\n",
        "\n",
        "#testing\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for x,y in testloader:\n",
        "\n",
        "        ypred = model(x)\n",
        "\n",
        "        _, predicted = torch.max(ypred.data, 1)\n",
        "\n",
        "        total += y.size(0)\n",
        "        correct += (predicted == y).sum().item()\n",
        "\n",
        "accuracy = 100 * correct / total\n",
        "print(f'Accuracy of the model on the test images: {accuracy:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P7ywKNAixEWN",
        "outputId": "52e58ff5-6b5d-4c98-c324-912a78cafae6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the model on the test images: 97.89%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#6\n",
        "\n",
        "#experiments with various hyperparameters\n",
        "\n",
        "learning_rates = [0.001, 0.01, 0.1]\n",
        "hl1 = 2\n",
        "hl2 = 3\n",
        "hl3 = 4\n",
        "\n",
        "best_accuracy = 0\n",
        "\n",
        "for rate in learning_rates:\n",
        "\n",
        "  #create model different number of hiddem layers\n",
        "  model1 = NET(input_size, hl1, hidden_size)\n",
        "  model2 = NET(input_size, hl2, hidden_size)\n",
        "  model3 = NET(input_size, hl3, hidden_size)\n",
        "\n",
        "  #new optim. with new learning rate\n",
        "  optimizer1 = optim.Adam(model1.parameters(), lr=rate)\n",
        "  optimizer2 = optim.Adam(model2.parameters(), lr=rate)\n",
        "  optimizer3 = optim.Adam(model3.parameters(), lr=rate)\n",
        "\n",
        "  #training the model\n",
        "\n",
        "  #using different number of epochs\n",
        "  for epoch in range(n_epochs):\n",
        "\n",
        "    for x,y in trainloader:\n",
        "\n",
        "      ypred1 = model1(x)\n",
        "      ypred2 = model2(x)\n",
        "      ypred3 = model3(x)\n",
        "      loss1 = loss_fn(ypred1, y)\n",
        "      loss2 = loss_fn(ypred2, y)\n",
        "      loss3 = loss_fn(ypred3, y)\n",
        "      optimizer1.zero_grad()\n",
        "      optimizer2.zero_grad()\n",
        "      optimizer3.zero_grad()\n",
        "      loss1.backward()\n",
        "      loss2.backward()\n",
        "      loss3.backward()\n",
        "      optimizer1.step()\n",
        "      optimizer2.step()\n",
        "      optimizer3.step()\n",
        "\n",
        "  #Testing the model\n",
        "\n",
        "  total = 0\n",
        "\n",
        "  correct1 = 0\n",
        "  correct2 = 0\n",
        "  correct3 = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for x,y in testloader:\n",
        "\n",
        "      ypred1 = model1(x)\n",
        "      ypred2 = model2(x)\n",
        "      ypred3 = model3(x)\n",
        "\n",
        "      _, predicted1 = torch.max(ypred1.data, 1)\n",
        "      _, predicted2 = torch.max(ypred2.data, 1)\n",
        "      _, predicted3 = torch.max(ypred3.data, 1)\n",
        "\n",
        "      total += y.size(0)\n",
        "      correct1 += (predicted1 == y).sum().item()\n",
        "      correct2 += (predicted2 == y).sum().item()\n",
        "      correct3 += (predicted3 == y).sum().item()\n",
        "\n",
        "  accuracy1 = 100 * correct1 / total\n",
        "  accuracy2 = 100 * correct2 / total\n",
        "  accuracy3 = 100 * correct3 / total\n",
        "\n",
        "\n",
        "  if accuracy1 > best_accuracy:\n",
        "    best_accuracy = accuracy1\n",
        "    best_set_of_parameters = [rate, n, hl1]\n",
        "\n",
        "  elif accuracy2 > best_accuracy:\n",
        "    best_accuracy = accuracy2\n",
        "    best_set_of_parameters = [rate, n, hl2]\n",
        "\n",
        "  elif accuracy3 > best_accuracy:\n",
        "    best_accuracy = accuracy3\n",
        "    best_set_of_parameters = [rate, hl3]\n",
        "\n",
        "print(f'Best Accuracy: {best_accuracy:.2f}%, optimal set of hyperparameters: Learning Rate: {best_set_of_parameters[0]}; Hidden layers: {best_set_of_parameters[1]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vdJfzMUX3KwT",
        "outputId": "64937060-00a8-41ed-85ef-6ad0ad33e51e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Accuracy: 96.58%, optimal set of hyperparameters: Learning Rate: 0.001; Hidden layers: 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#If we want we can also test with different numbers of epochs and learning rates\n",
        "#with the following code. I didn't run it beacuse of the very long time it would need\n",
        "\n",
        "\n",
        "learning_rates = [0.001, 0.01, 0.1]\n",
        "epoch_list = [5, 10, 15]\n",
        "hl1 = 2\n",
        "hl2 = 3\n",
        "hl3 = 4\n",
        "\n",
        "best_accuracy = 0\n",
        "\n",
        "for rate in learning_rates:\n",
        "\n",
        "  #create model different number of hiddem layers\n",
        "  model1 = NET(input_size, hl1, hidden_size)\n",
        "  model2 = NET(input_size, hl2, hidden_size)\n",
        "  model3 = NET(input_size, hl3, hidden_size)\n",
        "\n",
        "  #new optim. with new learning rate\n",
        "  optimizer1 = optim.Adam(model1.parameters(), lr=rate)\n",
        "  optimizer2 = optim.Adam(model2.parameters(), lr=rate)\n",
        "  optimizer3 = optim.Adam(model3.parameters(), lr=rate)\n",
        "\n",
        "  for n in epoch_list:\n",
        "\n",
        "\n",
        "    #training the model\n",
        "\n",
        "    #using different number of epochs\n",
        "    for epoch in range(n):\n",
        "\n",
        "      for x,y in trainloader:\n",
        "\n",
        "        ypred1 = model1(x)\n",
        "        ypred2 = model2(x)\n",
        "        ypred3 = model3(x)\n",
        "        loss1 = loss_fn(ypred1, y)\n",
        "        loss2 = loss_fn(ypred2, y)\n",
        "        loss3 = loss_fn(ypred3, y)\n",
        "        optimizer1.zero_grad()\n",
        "        optimizer2.zero_grad()\n",
        "        optimizer3.zero_grad()\n",
        "        loss1.backward()\n",
        "        loss2.backward()\n",
        "        loss3.backward()\n",
        "        optimizer1.step()\n",
        "        optimizer2.step()\n",
        "        optimizer3.step()\n",
        "\n",
        "    #Testing the model\n",
        "\n",
        "    total = 0\n",
        "\n",
        "    correct1 = 0\n",
        "    correct2 = 0\n",
        "    correct3 = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for x,y in testloader:\n",
        "\n",
        "        ypred1 = model1(x)\n",
        "        ypred2 = model2(x)\n",
        "        ypred3 = model3(x)\n",
        "\n",
        "        _, predicted1 = torch.max(ypred1.data, 1)\n",
        "        _, predicted2 = torch.max(ypred2.data, 1)\n",
        "        _, predicted3 = torch.max(ypred3.data, 1)\n",
        "\n",
        "        total += y.size(0)\n",
        "        correct1 += (predicted1 == y).sum().item()\n",
        "        correct2 += (predicted2 == y).sum().item()\n",
        "        correct3 += (predicted3 == y).sum().item()\n",
        "\n",
        "    accuracy1 = 100 * correct1 / total\n",
        "    accuracy2 = 100 * correct2 / total\n",
        "    accuracy3 = 100 * correct3 / total\n",
        "\n",
        "\n",
        "    if accuracy1 > best_accuracy:\n",
        "      best_accuracy = accuracy1\n",
        "      best_set_of_parameters = [rate, n, hl1]\n",
        "\n",
        "    elif accuracy2 > best_accuracy:\n",
        "      best_accuracy = accuracy2\n",
        "      best_set_of_parameters = [rate, n, hl2]\n",
        "\n",
        "    elif accuracy3 > best_accuracy:\n",
        "      best_accuracy = accuracy3\n",
        "      best_set_of_parameters = [rate, n, hl3]\n",
        "\n",
        "print(f'Best Accuracy: {best_accuracy:.2f}%, optimal set of hyperparameters: Learning Rate: {best_set_of_parameters[0]}; Epochs: {best_set_of_parameters[1]}; Hidden layers: {best_set_of_parameters[2]}')"
      ],
      "metadata": {
        "id": "uVZfv8bGIkjN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}