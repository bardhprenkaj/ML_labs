{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Challenge: Implement a Multiclass Classification Neural Network using PyTorch**\n",
        "\n",
        "Objective:\n",
        "Build a neural network using PyTorch to predict handwritten digits of MNIST.\n",
        "\n",
        "Steps:\n",
        "\n",
        "1. **Data Preparation**: Load the MNIST dataset using ```torchvision.datasets.MNIST```. Standardize/normalize the features. Split the dataset into training and testing sets using, for example, ```sklearn.model_selection.train_test_split()```. **Bonus scores**: *use PyTorch's built-* ```DataLoader``` *to split the dataset*.\n",
        "\n",
        "2. **Neural Network Architecture**: Define a simple feedforward neural network using PyTorch's ```nn.Module```. Design the input layer to match the number of features in the MNIST dataset and the output layer to have as many neurons as there are classes (10). You can experiment with the number of hidden layers and neurons to optimize the performance. **Bonus scores**: *Make your architecture flexibile to have as many hidden layers as the user wants, and use hyperparameter optimization to select the best number of hidden layeres.*\n",
        "\n",
        "3. **Loss Function and Optimizer**: Choose an appropriate loss function for multiclass classification. Select an optimizer, like SGD (Stochastic Gradient Descent) or Adam.\n",
        "\n",
        "4. **Training**: Write a training loop to iterate over the dataset.\n",
        "Forward pass the input through the network, calculate the loss, and perform backpropagation. Update the weights of the network using the chosen optimizer.\n",
        "\n",
        "5. **Testing**: Evaluate the trained model on the test set. Calculate the accuracy of the model.\n",
        "\n",
        "6. **Optimization**: Experiment with hyperparameters (learning rate, number of epochs, etc.) to optimize the model's performance. Consider adjusting the neural network architecture for better results. **Notice that you can't use the optimization algorithms from scikit-learn that we saw in lab1: e.g.,** ```GridSearchCV```.\n"
      ],
      "metadata": {
        "id": "E5AYt3Cn35zA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Data Preparation**"
      ],
      "metadata": {
        "id": "CaOxq7P60dfG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5XLynrxJ33v6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2042ff10-35bc-4e46-895c-564664bd2cf1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 83337598.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 67560342.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 28223059.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 10830317.66it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# insert code here\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
        "import numpy as np\n",
        "\n",
        "# Definisci le trasformazioni (compresa la normalizzazione)\n",
        "#In questo codice, transforms.Normalize((0.5,), (0.5,)) applica una normalizzazione a ciascuna immagine del dataset.\n",
        "#Questa trasformazione modifica i valori dei pixel in modo che abbiano una media di 0.5 e una deviazione standard di 0.5.\n",
        "#È un passaggio comune nella preparazione dei dati, specialmente per i dati delle immagini, per migliorare\n",
        "#la stabilità e le performance del processo di allenamento della rete neurale.#\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# Carica il dataset MNIST\n",
        "dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "\n",
        "# Calcola gli indici per lo split\n",
        "dataset_size = len(dataset)\n",
        "indices = list(range(dataset_size))\n",
        "split = int(np.floor(0.2 * dataset_size))  # ad esempio, 20% per la validazione\n",
        "np.random.shuffle(indices)\n",
        "\n",
        "train_indices, val_indices = indices[split:], indices[:split]\n",
        "\n",
        "# Crea samplers per il training e la validazione\n",
        "train_sampler = SubsetRandomSampler(train_indices)\n",
        "validation_sampler = SubsetRandomSampler(val_indices)\n",
        "\n",
        "# Crea DataLoader per il training e la validazione\n",
        "train_loader = DataLoader(dataset, batch_size=64, sampler=train_sampler)\n",
        "validation_loader = DataLoader(dataset, batch_size=64, sampler=validation_sampler)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Neural Network Architecture**\n"
      ],
      "metadata": {
        "id": "nCPeFLwn04Pk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class FlexibleFeedforwardNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_sizes, output_size):\n",
        "        super(FlexibleFeedforwardNN, self).__init__()\n",
        "\n",
        "        # Create a list to hold the layers\n",
        "        self.layers = nn.ModuleList()\n",
        "\n",
        "        # Add the first layer (input layer)\n",
        "        self.layers.append(nn.Linear(input_size, hidden_sizes[0]))\n",
        "\n",
        "        # Add hidden layers, if more than one size is given\n",
        "        for i in range(1, len(hidden_sizes)):\n",
        "            self.layers.append(nn.Linear(hidden_sizes[i - 1], hidden_sizes[i]))\n",
        "\n",
        "        # Add the output layer\n",
        "        self.layers.append(nn.Linear(hidden_sizes[-1], output_size))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Flatten the input tensor\n",
        "        #L'uso di x.view(x.size(0), -1) trasforma quindi il tensore x in una forma in cui ogni elemento del batch è un vettore unidimensionale.\n",
        "        #Ad esempio, se x è un tensore che rappresenta un batch di immagini MNIST (con dimensione del batch, ad esempio, 64) dove ogni immagine\n",
        "        #è 28x28 pixel, x avrà inizialmente una forma di [64, 28, 28].\n",
        "        #Dopo l'applicazione di x.view(x.size(0), -1), la forma di x diventerà [64, 784],\n",
        "        #dove 784 è il risultato dell'appiattimento delle dimensioni 28x28 delle immagini.\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        # Pass through each layer except for the last one with activation function\n",
        "        #x = F.relu(layer(x))\n",
        "        #In questo ciclo, per ogni strato (tranne l'ultimo), si esegue una trasformazione lineare seguita da una funzione di attivazione ReLu\n",
        "        #In breve, layer(x) è una trasformazione lineare perché layer è un oggetto nn.Linear,\n",
        "        # e la chiamata layer(x) invoca il metodo forward di nn.Linear, che implementa la trasformazione lineare.\n",
        "        for layer in self.layers[:-1]:\n",
        "            x = F.relu(layer(x))\n",
        "\n",
        "        # No activation function for the last layer (output layer)\n",
        "        x = self.layers[-1](x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "SiEesxfG0zem"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of features for the MNIST dataset\n",
        "input_size = 784  # 28x28 images\n",
        "# Example hidden layer sizes\n",
        "# Questo approccio rende la rete neurale flessibile, poiché puoi facilmente modificare\n",
        "# il numero e le dimensioni degli strati nascosti modificando semplicemente questa lista.\n",
        "# Ad esempio, se vuoi una rete con tre strati nascosti di dimensioni 128, 64 e 32,\n",
        "#potresti impostare hidden_sizes = [128, 64, 32].\n",
        "hidden_sizes = [128, 64]\n",
        "# Number of classes in the MNIST dataset\n",
        "output_size = 10\n",
        "\n",
        "# Create the neural network\n",
        "net = FlexibleFeedforwardNN(input_size, hidden_sizes, output_size)\n"
      ],
      "metadata": {
        "id": "tMhR4PIg5d9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. **Loss Function and Optimizer**"
      ],
      "metadata": {
        "id": "oOz_UHGDB8pR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "tE_HuQF3CDAP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assume 'net' is the instance of your neural network\n",
        "# For example: net = FlexibleFeedforwardNN(input_size, hidden_sizes, output_size)\n",
        "\n",
        "# Define the loss function (Cross-Entropy Loss)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Define the optimizer (You can choose between SGD, Adam, etc.)\n",
        "# Using Adam optimizer\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
        "\n",
        "# Or, using SGD optimizer\n",
        "# optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9)\n"
      ],
      "metadata": {
        "id": "EPaUcxq2CZ66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. **Training**"
      ],
      "metadata": {
        "id": "5BjvjXziDvaM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming the following have been defined:\n",
        "# net - instance of the neural network\n",
        "# criterion - loss function\n",
        "# optimizer - optimization algorithm\n",
        "# train_loader - DataLoader with training data\n",
        "\n",
        "num_epochs = 5  # Number of training epochs\n",
        "\n",
        "for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        # get the inputs; data is a tuple of (inputs, labels)\n",
        "        inputs, labels = data\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward pass\n",
        "        outputs = net(inputs)\n",
        "\n",
        "        # calculate the loss\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # backward pass (backpropagation)\n",
        "        loss.backward()\n",
        "\n",
        "        # update the weights\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 200 == 199:    # print every 200 mini-batches\n",
        "            print(f'[{epoch + 1}, {i + 1}] loss: {running_loss / 200:.3f}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uRjaUNjvEFpk",
        "outputId": "929e928d-ff33-481e-9bc4-9698a5f86f2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 200] loss: 0.735\n",
            "[1, 400] loss: 0.369\n",
            "[1, 600] loss: 0.310\n",
            "[2, 200] loss: 0.230\n",
            "[2, 400] loss: 0.223\n",
            "[2, 600] loss: 0.204\n",
            "[3, 200] loss: 0.161\n",
            "[3, 400] loss: 0.169\n",
            "[3, 600] loss: 0.148\n",
            "[4, 200] loss: 0.130\n",
            "[4, 400] loss: 0.119\n",
            "[4, 600] loss: 0.126\n",
            "[5, 200] loss: 0.104\n",
            "[5, 400] loss: 0.093\n",
            "[5, 600] loss: 0.109\n",
            "Finished Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5. Testing**"
      ],
      "metadata": {
        "id": "JhkZQJtJFwXl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the model to evaluation mode\n",
        "net.eval()\n",
        "\n",
        "# Initialize the counters\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "# Disable gradient calculations\n",
        "with torch.no_grad():\n",
        "    for data in validation_loader:\n",
        "        images, labels = data\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = net(images)\n",
        "\n",
        "        # Get the predicted class with the highest score\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "        # Update the counters\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "# Calculate the accuracy\n",
        "accuracy = 100 * correct / total\n",
        "print(f'Accuracy of the model on the test images: {accuracy:.2f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4iznjz7F1ij",
        "outputId": "d38d4d8a-0390-4eae-a526-62ee53e08fbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the model on the test images: 96.54%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **6. Optimization**"
      ],
      "metadata": {
        "id": "lHfsxINlINKF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# Define a list of learning rates and epochs to try\n",
        "learning_rates = [0.001, 0.01, 0.1]\n",
        "num_epochs_options = [5, 10, 15]\n",
        "\n",
        "# Store the best accuracy found\n",
        "best_accuracy = 0\n",
        "best_lr = None\n",
        "best_epoch = None\n",
        "\n",
        "for lr in learning_rates:\n",
        "    for num_epochs in num_epochs_options:\n",
        "        # Initialize the neural network\n",
        "        net = FlexibleFeedforwardNN(input_size, hidden_sizes, output_size)\n",
        "\n",
        "        # Define the loss function and optimizer\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = optim.Adam(net.parameters(), lr=lr)\n",
        "\n",
        "        # Training loop\n",
        "\n",
        "        for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
        "            running_loss = 0.0\n",
        "\n",
        "            for i, data in enumerate(train_loader, 0):\n",
        "                # get the inputs; data is a tuple of (inputs, labels)\n",
        "                  inputs, labels = data\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                  optimizer.zero_grad()\n",
        "\n",
        "                # forward pass\n",
        "                  outputs = net(inputs)\n",
        "\n",
        "                # calculate the loss\n",
        "                  loss = criterion(outputs, labels)\n",
        "\n",
        "                # backward pass (backpropagation)\n",
        "                  loss.backward()\n",
        "\n",
        "                # update the weights\n",
        "                  optimizer.step()\n",
        "\n",
        "                # print statistics\n",
        "                  running_loss += loss.item()\n",
        "                  if i % 200 == 199:    # print every 200 mini-batches\n",
        "                    print(f'[{epoch + 1}, {i + 1}] loss: {running_loss / 200:.3f}')\n",
        "                    running_loss = 0.0\n",
        "\n",
        "        print('Finished Training')\n",
        "\n",
        "\n",
        "        # Testing the model\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for data in validation_loader:\n",
        "                images, labels = data\n",
        "                outputs = net(images)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        accuracy = 100 * correct / total\n",
        "        print(f'LR: {lr}, Epochs: {num_epochs}, Accuracy: {accuracy:.2f}%')\n",
        "\n",
        "        # Save the best model\n",
        "        if accuracy > best_accuracy:\n",
        "            best_accuracy = accuracy\n",
        "            best_lr = lr\n",
        "            best_epoch = num_epochs\n",
        "\n",
        "    print(f'Best Accuracy: {best_accuracy:.2f}%, Learning Rate: {best_lr}, Epochs: {best_epoch}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kjxMHzCkIS00",
        "outputId": "647e6ee9-a376-4937-ef42-26ca85ecac54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 200] loss: 0.774\n",
            "[1, 400] loss: 0.364\n",
            "[1, 600] loss: 0.310\n",
            "[2, 200] loss: 0.238\n",
            "[2, 400] loss: 0.216\n",
            "[2, 600] loss: 0.206\n",
            "[3, 200] loss: 0.170\n",
            "[3, 400] loss: 0.165\n",
            "[3, 600] loss: 0.156\n",
            "[4, 200] loss: 0.139\n",
            "[4, 400] loss: 0.126\n",
            "[4, 600] loss: 0.129\n",
            "[5, 200] loss: 0.108\n",
            "[5, 400] loss: 0.105\n",
            "[5, 600] loss: 0.112\n",
            "Finished Training\n",
            "LR: 0.001, Epochs: 5, Accuracy: 95.50%\n",
            "[1, 200] loss: 0.724\n",
            "[1, 400] loss: 0.354\n",
            "[1, 600] loss: 0.333\n",
            "[2, 200] loss: 0.244\n",
            "[2, 400] loss: 0.219\n",
            "[2, 600] loss: 0.193\n",
            "[3, 200] loss: 0.168\n",
            "[3, 400] loss: 0.145\n",
            "[3, 600] loss: 0.158\n",
            "[4, 200] loss: 0.119\n",
            "[4, 400] loss: 0.121\n",
            "[4, 600] loss: 0.122\n",
            "[5, 200] loss: 0.103\n",
            "[5, 400] loss: 0.104\n",
            "[5, 600] loss: 0.106\n",
            "[6, 200] loss: 0.093\n",
            "[6, 400] loss: 0.094\n",
            "[6, 600] loss: 0.084\n",
            "[7, 200] loss: 0.077\n",
            "[7, 400] loss: 0.078\n",
            "[7, 600] loss: 0.083\n",
            "[8, 200] loss: 0.063\n",
            "[8, 400] loss: 0.076\n",
            "[8, 600] loss: 0.071\n",
            "[9, 200] loss: 0.071\n",
            "[9, 400] loss: 0.062\n",
            "[9, 600] loss: 0.067\n",
            "[10, 200] loss: 0.051\n",
            "[10, 400] loss: 0.058\n",
            "[10, 600] loss: 0.069\n",
            "Finished Training\n",
            "LR: 0.001, Epochs: 10, Accuracy: 96.26%\n",
            "[1, 200] loss: 0.707\n",
            "[1, 400] loss: 0.358\n",
            "[1, 600] loss: 0.295\n",
            "[2, 200] loss: 0.228\n",
            "[2, 400] loss: 0.214\n",
            "[2, 600] loss: 0.186\n",
            "[3, 200] loss: 0.156\n",
            "[3, 400] loss: 0.149\n",
            "[3, 600] loss: 0.139\n",
            "[4, 200] loss: 0.126\n",
            "[4, 400] loss: 0.127\n",
            "[4, 600] loss: 0.119\n",
            "[5, 200] loss: 0.101\n",
            "[5, 400] loss: 0.091\n",
            "[5, 600] loss: 0.109\n",
            "[6, 200] loss: 0.086\n",
            "[6, 400] loss: 0.085\n",
            "[6, 600] loss: 0.089\n",
            "[7, 200] loss: 0.077\n",
            "[7, 400] loss: 0.076\n",
            "[7, 600] loss: 0.074\n",
            "[8, 200] loss: 0.069\n",
            "[8, 400] loss: 0.067\n",
            "[8, 600] loss: 0.078\n",
            "[9, 200] loss: 0.057\n",
            "[9, 400] loss: 0.059\n",
            "[9, 600] loss: 0.065\n",
            "[10, 200] loss: 0.052\n",
            "[10, 400] loss: 0.055\n",
            "[10, 600] loss: 0.064\n",
            "[11, 200] loss: 0.052\n",
            "[11, 400] loss: 0.050\n",
            "[11, 600] loss: 0.051\n",
            "[12, 200] loss: 0.037\n",
            "[12, 400] loss: 0.050\n",
            "[12, 600] loss: 0.050\n",
            "[13, 200] loss: 0.036\n",
            "[13, 400] loss: 0.043\n",
            "[13, 600] loss: 0.047\n",
            "[14, 200] loss: 0.035\n",
            "[14, 400] loss: 0.043\n",
            "[14, 600] loss: 0.043\n",
            "[15, 200] loss: 0.036\n",
            "[15, 400] loss: 0.026\n",
            "[15, 600] loss: 0.042\n",
            "Finished Training\n",
            "LR: 0.001, Epochs: 15, Accuracy: 96.61%\n",
            "Best Accuracy: 96.61%, Learning Rate: 0.001, Epochs: 15\n",
            "[1, 200] loss: 0.656\n",
            "[1, 400] loss: 0.355\n",
            "[1, 600] loss: 0.314\n",
            "[2, 200] loss: 0.262\n",
            "[2, 400] loss: 0.251\n",
            "[2, 600] loss: 0.257\n",
            "[3, 200] loss: 0.225\n",
            "[3, 400] loss: 0.248\n",
            "[3, 600] loss: 0.249\n",
            "[4, 200] loss: 0.229\n",
            "[4, 400] loss: 0.222\n",
            "[4, 600] loss: 0.218\n",
            "[5, 200] loss: 0.193\n",
            "[5, 400] loss: 0.234\n",
            "[5, 600] loss: 0.223\n",
            "Finished Training\n",
            "LR: 0.01, Epochs: 5, Accuracy: 93.56%\n",
            "[1, 200] loss: 0.664\n",
            "[1, 400] loss: 0.373\n",
            "[1, 600] loss: 0.308\n",
            "[2, 200] loss: 0.284\n",
            "[2, 400] loss: 0.281\n",
            "[2, 600] loss: 0.267\n",
            "[3, 200] loss: 0.220\n",
            "[3, 400] loss: 0.234\n",
            "[3, 600] loss: 0.249\n",
            "[4, 200] loss: 0.231\n",
            "[4, 400] loss: 0.243\n",
            "[4, 600] loss: 0.254\n",
            "[5, 200] loss: 0.234\n",
            "[5, 400] loss: 0.228\n",
            "[5, 600] loss: 0.219\n",
            "[6, 200] loss: 0.202\n",
            "[6, 400] loss: 0.226\n",
            "[6, 600] loss: 0.232\n",
            "[7, 200] loss: 0.203\n",
            "[7, 400] loss: 0.206\n",
            "[7, 600] loss: 0.224\n",
            "[8, 200] loss: 0.197\n",
            "[8, 400] loss: 0.222\n",
            "[8, 600] loss: 0.244\n",
            "[9, 200] loss: 0.185\n",
            "[9, 400] loss: 0.186\n",
            "[9, 600] loss: 0.210\n",
            "[10, 200] loss: 0.197\n",
            "[10, 400] loss: 0.188\n",
            "[10, 600] loss: 0.210\n",
            "Finished Training\n",
            "LR: 0.01, Epochs: 10, Accuracy: 92.64%\n",
            "[1, 200] loss: 0.670\n",
            "[1, 400] loss: 0.357\n",
            "[1, 600] loss: 0.291\n",
            "[2, 200] loss: 0.257\n",
            "[2, 400] loss: 0.249\n",
            "[2, 600] loss: 0.253\n",
            "[3, 200] loss: 0.227\n",
            "[3, 400] loss: 0.224\n",
            "[3, 600] loss: 0.244\n",
            "[4, 200] loss: 0.204\n",
            "[4, 400] loss: 0.235\n",
            "[4, 600] loss: 0.234\n",
            "[5, 200] loss: 0.209\n",
            "[5, 400] loss: 0.224\n",
            "[5, 600] loss: 0.225\n",
            "[6, 200] loss: 0.229\n",
            "[6, 400] loss: 0.202\n",
            "[6, 600] loss: 0.197\n",
            "[7, 200] loss: 0.210\n",
            "[7, 400] loss: 0.223\n",
            "[7, 600] loss: 0.215\n",
            "[8, 200] loss: 0.193\n",
            "[8, 400] loss: 0.183\n",
            "[8, 600] loss: 0.225\n",
            "[9, 200] loss: 0.190\n",
            "[9, 400] loss: 0.228\n",
            "[9, 600] loss: 0.189\n",
            "[10, 200] loss: 0.190\n",
            "[10, 400] loss: 0.210\n",
            "[10, 600] loss: 0.203\n",
            "[11, 200] loss: 0.183\n",
            "[11, 400] loss: 0.204\n",
            "[11, 600] loss: 0.208\n",
            "[12, 200] loss: 0.195\n",
            "[12, 400] loss: 0.180\n",
            "[12, 600] loss: 0.172\n",
            "[13, 200] loss: 0.164\n",
            "[13, 400] loss: 0.207\n",
            "[13, 600] loss: 0.188\n",
            "[14, 200] loss: 0.168\n",
            "[14, 400] loss: 0.176\n",
            "[14, 600] loss: 0.189\n",
            "[15, 200] loss: 0.201\n",
            "[15, 400] loss: 0.179\n",
            "[15, 600] loss: 0.198\n",
            "Finished Training\n",
            "LR: 0.01, Epochs: 15, Accuracy: 95.13%\n",
            "Best Accuracy: 96.61%, Learning Rate: 0.001, Epochs: 15\n",
            "[1, 200] loss: 5.245\n",
            "[1, 400] loss: 2.309\n",
            "[1, 600] loss: 2.308\n",
            "[2, 200] loss: 2.310\n",
            "[2, 400] loss: 2.310\n",
            "[2, 600] loss: 2.309\n",
            "[3, 200] loss: 2.311\n",
            "[3, 400] loss: 2.309\n",
            "[3, 600] loss: 2.310\n",
            "[4, 200] loss: 2.309\n",
            "[4, 400] loss: 2.309\n",
            "[4, 600] loss: 2.311\n",
            "[5, 200] loss: 2.309\n",
            "[5, 400] loss: 2.313\n",
            "[5, 600] loss: 2.310\n",
            "Finished Training\n",
            "LR: 0.1, Epochs: 5, Accuracy: 10.43%\n",
            "[1, 200] loss: 6.558\n",
            "[1, 400] loss: 2.309\n",
            "[1, 600] loss: 2.308\n",
            "[2, 200] loss: 2.309\n",
            "[2, 400] loss: 2.308\n",
            "[2, 600] loss: 2.308\n",
            "[3, 200] loss: 2.310\n",
            "[3, 400] loss: 2.308\n",
            "[3, 600] loss: 2.309\n",
            "[4, 200] loss: 2.309\n",
            "[4, 400] loss: 2.311\n",
            "[4, 600] loss: 2.311\n",
            "[5, 200] loss: 2.310\n",
            "[5, 400] loss: 2.312\n",
            "[5, 600] loss: 2.310\n",
            "[6, 200] loss: 2.310\n",
            "[6, 400] loss: 2.311\n",
            "[6, 600] loss: 2.311\n",
            "[7, 200] loss: 2.311\n",
            "[7, 400] loss: 2.310\n",
            "[7, 600] loss: 2.311\n",
            "[8, 200] loss: 2.310\n",
            "[8, 400] loss: 2.310\n",
            "[8, 600] loss: 2.309\n",
            "[9, 200] loss: 2.310\n",
            "[9, 400] loss: 2.310\n",
            "[9, 600] loss: 2.313\n",
            "[10, 200] loss: 2.311\n",
            "[10, 400] loss: 2.313\n",
            "[10, 600] loss: 2.308\n",
            "Finished Training\n",
            "LR: 0.1, Epochs: 10, Accuracy: 10.03%\n",
            "[1, 200] loss: 4.681\n",
            "[1, 400] loss: 2.308\n",
            "[1, 600] loss: 2.309\n",
            "[2, 200] loss: 2.310\n",
            "[2, 400] loss: 2.309\n",
            "[2, 600] loss: 2.310\n",
            "[3, 200] loss: 2.308\n",
            "[3, 400] loss: 2.309\n",
            "[3, 600] loss: 2.310\n",
            "[4, 200] loss: 2.309\n",
            "[4, 400] loss: 2.311\n",
            "[4, 600] loss: 2.310\n",
            "[5, 200] loss: 2.310\n",
            "[5, 400] loss: 2.309\n",
            "[5, 600] loss: 2.310\n",
            "[6, 200] loss: 2.310\n",
            "[6, 400] loss: 2.309\n",
            "[6, 600] loss: 2.314\n",
            "[7, 200] loss: 2.312\n",
            "[7, 400] loss: 2.309\n",
            "[7, 600] loss: 2.308\n",
            "[8, 200] loss: 2.310\n",
            "[8, 400] loss: 2.310\n",
            "[8, 600] loss: 2.310\n",
            "[9, 200] loss: 2.310\n",
            "[9, 400] loss: 2.310\n",
            "[9, 600] loss: 2.310\n",
            "[10, 200] loss: 2.311\n",
            "[10, 400] loss: 2.310\n",
            "[10, 600] loss: 2.309\n",
            "[11, 200] loss: 2.310\n",
            "[11, 400] loss: 2.311\n",
            "[11, 600] loss: 2.312\n",
            "[12, 200] loss: 2.309\n",
            "[12, 400] loss: 2.311\n",
            "[12, 600] loss: 2.312\n",
            "[13, 200] loss: 2.313\n",
            "[13, 400] loss: 2.309\n",
            "[13, 600] loss: 2.309\n",
            "[14, 200] loss: 2.308\n",
            "[14, 400] loss: 2.313\n",
            "[14, 600] loss: 2.311\n",
            "[15, 200] loss: 2.309\n",
            "[15, 400] loss: 2.310\n",
            "[15, 600] loss: 2.309\n",
            "Finished Training\n",
            "LR: 0.1, Epochs: 15, Accuracy: 9.79%\n",
            "Best Accuracy: 96.61%, Learning Rate: 0.001, Epochs: 15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Per iterare anche sul numero di layer bisognerebbe aggiungere il seguente codice .\n",
        "Per brevità di computazione si tralascia di eseguire anche questo step, che è ovvio."
      ],
      "metadata": {
        "id": "Q1K-rkfrRTYh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# Definisci una lista di configurazioni per gli strati nascosti\n",
        "hidden_layers_configurations = [\n",
        "    [128, 64],  # Rete con 2 strati nascosti (128 neuroni e 64 neuroni)\n",
        "    [128, 128, 64],  # Rete con 3 strati nascosti\n",
        "    [256, 128, 64, 32]  # Rete con 4 strati nascosti\n",
        "    # ... altre configurazioni ...\n",
        "]\n",
        "\n",
        "# Definisci una lista di learning rates e epoche da provare\n",
        "learning_rates = [0.001, 0.01, 0.1]\n",
        "num_epochs_options = [5, 10, 15]\n",
        "\n",
        "# Inizializza le variabili per tenere traccia della migliore configurazione\n",
        "best_accuracy = 0\n",
        "best_config = None\n",
        "\n",
        "for hidden_sizes in hidden_layers_configurations:\n",
        "    for lr in learning_rates:\n",
        "        for num_epochs in num_epochs_options:\n",
        "            # Inizializza la rete neurale con la configurazione corrente\n",
        "            net = FlexibleFeedforwardNN(input_size, hidden_sizes, output_size)\n",
        "\n",
        "            # Definisci la funzione di perdita e l'ottimizzatore\n",
        "            criterion = nn.CrossEntropyLoss()\n",
        "            optimizer = optim.Adam(net.parameters(), lr=lr)\n",
        "\n",
        "            # Ciclo di training\n",
        "            for epoch in range(num_epochs):\n",
        "                # ... (passaggi del training) ...\n",
        "                pass  # Sostituisci con il codice del ciclo di training\n",
        "\n",
        "            # Valutazione del modello\n",
        "            # ... (passaggi del testing) ...\n",
        "            # Calcola l'accuratezza e aggiorna la migliore configurazione se necessario\n",
        "\n",
        "            if accuracy > best_accuracy:\n",
        "                best_accuracy = accuracy\n",
        "                best_config = (hidden_sizes, lr, num_epochs)\n",
        "\n",
        "print(f'Best Accuracy: {best_accuracy:.2f}%, Configuration: {best_config}')\n"
      ],
      "metadata": {
        "id": "6ombQsDBRcLL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}