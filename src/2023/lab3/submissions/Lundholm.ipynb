{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5AYt3Cn35zA"
      },
      "source": [
        "**Challenge: Implement a Multiclass Classification Neural Network using PyTorch**\n",
        "\n",
        "Objective:\n",
        "Build a feedforward neural network using PyTorch to predict the species of iris flowers in a multiclass classification problem. The dataset used for this challenge is the Iris dataset, which consists of features like sepal length, sepal width, petal length, and petal width.\n",
        "\n",
        "Steps:\n",
        "\n",
        "1. **Data Preparation**: Load the MNIST dataset using ```torchvision.datasets.MNIST```. Standardize/normalize the features. Split the dataset into training and testing sets using, for example, ```sklearn.model_selection.train_test_split()```. **Bonus scores**: *use PyTorch's built-* ```DataLoader``` *to split the dataset*.\n",
        "\n",
        "2. **Neural Network Architecture**: Define a simple feedforward neural network using PyTorch's ```nn.Module```. Design the input layer to match the number of features in the MNIST dataset and the output layer to have as many neurons as there are classes (10). You can experiment with the number of hidden layers and neurons to optimize the performance. **Bonus scores**: *Make your architecture flexibile to have as many hidden layers as the user wants, and use hyperparameter optimization to select the best number of hidden layeres.*\n",
        "\n",
        "3. **Loss Function and Optimizer**: Choose an appropriate loss function for multiclass classification. Select an optimizer, like SGD (Stochastic Gradient Descent) or Adam.\n",
        "\n",
        "4. **Training**: Write a training loop to iterate over the dataset.\n",
        "Forward pass the input through the network, calculate the loss, and perform backpropagation. Update the weights of the network using the chosen optimizer.\n",
        "\n",
        "5. **Testing**: Evaluate the trained model on the test set. Calculate the accuracy of the model.\n",
        "\n",
        "6. **Optimization**: Experiment with hyperparameters (learning rate, number of epochs, etc.) to optimize the model's performance. Consider adjusting the neural network architecture for better results. **Notice that you can't use the optimization algorithms from scikit-learn that we saw in lab1: e.g.,** ```GridSearchCV```.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "5XLynrxJ33v6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Image shape: torch.Size([1, 28, 28])\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torch\n",
        "\n",
        "transform= torchvision.transforms.Compose([torchvision.transforms.ToTensor(), torchvision.transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "trainset=torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "\n",
        "testset=torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform )\n",
        "\n",
        "# Get a sample image\n",
        "sample_image, _ = trainset[0]\n",
        "\n",
        "# Print the shape of the image\n",
        "print(\"Image shape:\", sample_image.shape)\n",
        "\n",
        "batch_size=64\n",
        "trainloader=torch.utils.data.DataLoader(trainset, batch_size=batch_size, drop_last=True, shuffle=True)\n",
        "\n",
        "testloader=torch.utils.data.DataLoader(testset, batch_size=batch_size, drop_last=True, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MNISTNN(\n",
            "  (hidden_layers): ModuleList(\n",
            "    (0): Linear(in_features=784, out_features=128, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=128, out_features=64, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=64, out_features=32, bias=True)\n",
            "    (5): ReLU()\n",
            "  )\n",
            "  (output_layer): Linear(in_features=32, out_features=10, bias=True)\n",
            "  (output_act): Sigmoid()\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "class MNISTNN(nn.Module):\n",
        "    def __init__(self, hidden_layers=[]):\n",
        "        super().__init__()\n",
        "        \n",
        "         # List to store hidden layer modules\n",
        "        self.hidden_layers = nn.ModuleList()\n",
        "\n",
        "        # Input layer\n",
        "        input_size = 28 * 28\n",
        "        current_size = input_size\n",
        "\n",
        "        # Add hidden layers\n",
        "        for hidden_size in hidden_layers:\n",
        "            self.hidden_layers.append(nn.Linear(current_size, hidden_size))\n",
        "            self.hidden_layers.append(nn.ReLU())\n",
        "            current_size = hidden_size\n",
        "\n",
        "        # Output layer\n",
        "        self.output_layer = nn.Linear(current_size, 10)\n",
        "        self.output_act = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply hidden layers\n",
        "        for layer in self.hidden_layers:\n",
        "            x = layer(x)\n",
        "\n",
        "        # Output layer\n",
        "        x = self.output_layer(x)\n",
        "        x = self.output_act(x)\n",
        "        return x\n",
        "\n",
        "model =MNISTNN(hidden_layers=[128, 64, 32])\n",
        "print (model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5, Loss: 1.5343281030654907\n",
            "Epoch 2/5, Loss: 1.548176646232605\n",
            "Epoch 3/5, Loss: 1.528997778892517\n",
            "Epoch 4/5, Loss: 1.5280919075012207\n",
            "Epoch 5/5, Loss: 1.5000522136688232\n"
          ]
        }
      ],
      "source": [
        "#Loss fuction \n",
        "loss_fn = nn.CrossEntropyLoss()  \n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 5  # Adjust as needed\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()  # Set the model to training mode\n",
        "\n",
        "    for inputs, labels in trainloader:\n",
        "        optimizer.zero_grad()  # Zero the gradients\n",
        "        outputs = model(inputs.view(inputs.size(0), -1))  # Forward pass\n",
        "        loss = loss_fn(outputs, labels)  # Compute the loss\n",
        "        loss.backward()  # Backward pass\n",
        "        optimizer.step()  # Update weights\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy on the test set: 95.42%\n"
          ]
        }
      ],
      "source": [
        "# Testing the model on the test set\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():  # Disable gradient computation for evaluation\n",
        "    for inputs, labels in testloader:\n",
        "        outputs = model(inputs.view(inputs.size(0), -1))\n",
        "        _, predicted = torch.max(outputs, 1)  # Get the index of the max log-probability\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = correct / total\n",
        "print(f\"Accuracy on the test set: {100 * accuracy:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hyperparameters: [128, 64, 32], Learning Rate: 0.001, Epochs: 5, Accuracy: 95.19%\n",
            "Hyperparameters: [128, 64, 32], Learning Rate: 0.001, Epochs: 10, Accuracy: 96.04%\n",
            "Hyperparameters: [128, 64, 32], Learning Rate: 0.01, Epochs: 5, Accuracy: 12.43%\n",
            "Hyperparameters: [128, 64, 32], Learning Rate: 0.01, Epochs: 10, Accuracy: 21.38%\n",
            "Hyperparameters: [256, 128], Learning Rate: 0.001, Epochs: 5, Accuracy: 96.31%\n",
            "Hyperparameters: [256, 128], Learning Rate: 0.001, Epochs: 10, Accuracy: 97.18%\n",
            "Hyperparameters: [256, 128], Learning Rate: 0.01, Epochs: 5, Accuracy: 10.11%\n",
            "Hyperparameters: [256, 128], Learning Rate: 0.01, Epochs: 10, Accuracy: 10.34%\n",
            "Hyperparameters: [64, 64, 64], Learning Rate: 0.001, Epochs: 5, Accuracy: 94.01%\n",
            "Hyperparameters: [64, 64, 64], Learning Rate: 0.001, Epochs: 10, Accuracy: 95.97%\n",
            "Hyperparameters: [64, 64, 64], Learning Rate: 0.01, Epochs: 5, Accuracy: 14.00%\n",
            "Hyperparameters: [64, 64, 64], Learning Rate: 0.01, Epochs: 10, Accuracy: 9.77%\n",
            "\n",
            "Best Hyperparameters: {'hidden_layers': [256, 128], 'learning_rate': 0.001, 'num_epochs': 10}, Best Accuracy: 97.18%\n"
          ]
        }
      ],
      "source": [
        "def train_and_evaluate(hidden_layers, learning_rate, num_epochs):\n",
        "    # Create the model\n",
        "    model = MNISTNN(hidden_layers)\n",
        "    \n",
        "    # Loss function and optimizer\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        for inputs, labels in trainloader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs.view(inputs.size(0), -1))\n",
        "            loss = loss_fn(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    # Testing the model on the test set\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in testloader:\n",
        "            outputs = model(inputs.view(inputs.size(0), -1))\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = correct / total\n",
        "    return accuracy\n",
        "\n",
        "# Hyperparameter optimization\n",
        "hidden_layers_options = [[128, 64, 32], [256, 128], [64, 64, 64]]\n",
        "learning_rate_options = [0.001, 0.01]\n",
        "num_epochs_options = [5, 10]\n",
        "\n",
        "best_accuracy = 0\n",
        "best_hyperparameters = None\n",
        "\n",
        "# Iterate over hyperparameter options\n",
        "for hidden_layers in hidden_layers_options:\n",
        "    for learning_rate in learning_rate_options:\n",
        "        for num_epochs in num_epochs_options:\n",
        "            accuracy = train_and_evaluate(hidden_layers, learning_rate, num_epochs)\n",
        "            print(f\"Hyperparameters: {hidden_layers}, Learning Rate: {learning_rate}, Epochs: {num_epochs}, Accuracy: {100 * accuracy:.2f}%\")\n",
        "\n",
        "            # Update best hyperparameters if the current model has higher accuracy\n",
        "            if accuracy > best_accuracy:\n",
        "                best_accuracy = accuracy\n",
        "                best_hyperparameters = {\n",
        "                    'hidden_layers': hidden_layers,\n",
        "                    'learning_rate': learning_rate,\n",
        "                    'num_epochs': num_epochs\n",
        "                }\n",
        "\n",
        "print(f\"\\nBest Hyperparameters: {best_hyperparameters}, Best Accuracy: {100 * best_accuracy:.2f}%\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
