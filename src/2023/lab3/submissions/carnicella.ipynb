{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5AYt3Cn35zA"
      },
      "source": [
        "**Challenge: Implement a Multiclass Classification Neural Network using PyTorch**\n",
        "\n",
        "Objective:\n",
        "Build a neural network using PyTorch to predict handwritten digits of MNIST.\n",
        "\n",
        "Steps:\n",
        "\n",
        "1. **Data Preparation**: Load the MNIST dataset using ```torchvision.datasets.MNIST```. Standardize/normalize the features. Split the dataset into training and testing sets using, for example, ```sklearn.model_selection.train_test_split()```. **Bonus scores**: *use PyTorch's built-* ```DataLoader``` *to split the dataset*.\n",
        "\n",
        "2. **Neural Network Architecture**: Define a simple feedforward neural network using PyTorch's ```nn.Module```. Design the input layer to match the number of features in the MNIST dataset and the output layer to have as many neurons as there are classes (10). You can experiment with the number of hidden layers and neurons to optimize the performance. **Bonus scores**: *Make your architecture flexibile to have as many hidden layers as the user wants, and use hyperparameter optimization to select the best number of hidden layeres.*\n",
        "\n",
        "3. **Loss Function and Optimizer**: Choose an appropriate loss function for multiclass classification. Select an optimizer, like SGD (Stochastic Gradient Descent) or Adam.\n",
        "\n",
        "4. **Training**: Write a training loop to iterate over the dataset.\n",
        "Forward pass the input through the network, calculate the loss, and perform backpropagation. Update the weights of the network using the chosen optimizer.\n",
        "\n",
        "5. **Testing**: Evaluate the trained model on the test set. Calculate the accuracy of the model.\n",
        "\n",
        "6. **Optimization**: Experiment with hyperparameters (learning rate, number of epochs, etc.) to optimize the model's performance. Consider adjusting the neural network architecture for better results. **Notice that you can't use the optimization algorithms from scikit-learn that we saw in lab1: e.g.,** ```GridSearchCV```.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. **Data Preparation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "5XLynrxJ33v6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Step 1: Preparazione dei dati\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# Caricamento del dataset MNIST\n",
        "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Divisione del dataset di addestramento in set di addestramento e validazione\n",
        "train_data, val_data, train_labels, val_labels = train_test_split(train_dataset.data, train_dataset.targets, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**2. Neural Network Architecture**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Definizione della rete neurale\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self, input_size, output_size, hidden_layers):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        self.hidden_layers = hidden_layers\n",
        "\n",
        "        # Creazione dei moduli lineari e delle funzioni di attivazione per i livelli nascosti\n",
        "        self.hidden = nn.ModuleList([nn.Linear(self.input_size, self.hidden_layers[0])])\n",
        "        layer_sizes = zip(self.hidden_layers[:-1], self.hidden_layers[1:])\n",
        "        self.hidden.extend([nn.Linear(h1, h2) for h1, h2 in layer_sizes])\n",
        "\n",
        "        # Output layer\n",
        "        self.output = nn.Linear(self.hidden_layers[-1], self.output_size)\n",
        "\n",
        "        # Funzione di attivazione\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        for linear in self.hidden:\n",
        "            x = self.relu(linear(x))\n",
        "        x = self.output(x)\n",
        "        return x\n",
        "\n",
        "params = [\n",
        "    {\"lr\": 0.01, \"epochs\": 15, \"hidden_layers\": [64, 32]},\n",
        "    {\"lr\": 0.001, \"epochs\": 15, \"hidden_layers\": [64, 32]},\n",
        "    {\"lr\": 0.001, \"epochs\": 20, \"hidden_layers\": [64, 32]},\n",
        "    {\"lr\": 0.0001, \"epochs\": 20, \"hidden_layers\": [64, 32]}\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**3-4-5. Loss Function, Optimizer, Training and Testing**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running Test 1:\n",
            "Epoch [1/15], Step [1000/1875], Loss: 0.5031\n",
            "Epoch [2/15], Step [1000/1875], Loss: 0.3080\n",
            "Epoch [3/15], Step [1000/1875], Loss: 0.2934\n",
            "Epoch [4/15], Step [1000/1875], Loss: 0.2912\n",
            "Epoch [5/15], Step [1000/1875], Loss: 0.2752\n",
            "Epoch [6/15], Step [1000/1875], Loss: 0.2679\n",
            "Epoch [7/15], Step [1000/1875], Loss: 0.2598\n",
            "Epoch [8/15], Step [1000/1875], Loss: 0.2648\n",
            "Epoch [9/15], Step [1000/1875], Loss: 0.2669\n",
            "Epoch [10/15], Step [1000/1875], Loss: 0.2559\n",
            "Epoch [11/15], Step [1000/1875], Loss: 0.2592\n",
            "Epoch [12/15], Step [1000/1875], Loss: 0.2508\n",
            "Epoch [13/15], Step [1000/1875], Loss: 0.2442\n",
            "Epoch [14/15], Step [1000/1875], Loss: 0.2478\n",
            "Epoch [15/15], Step [1000/1875], Loss: 0.2373\n",
            "Accuracy on the test set: 92.61%\n",
            "==============================\n",
            "Running Test 2:\n",
            "Epoch [1/15], Step [1000/1875], Loss: 0.5095\n",
            "Epoch [2/15], Step [1000/1875], Loss: 0.2301\n",
            "Epoch [3/15], Step [1000/1875], Loss: 0.1721\n",
            "Epoch [4/15], Step [1000/1875], Loss: 0.1446\n",
            "Epoch [5/15], Step [1000/1875], Loss: 0.1199\n",
            "Epoch [6/15], Step [1000/1875], Loss: 0.1070\n",
            "Epoch [7/15], Step [1000/1875], Loss: 0.0992\n",
            "Epoch [8/15], Step [1000/1875], Loss: 0.0956\n",
            "Epoch [9/15], Step [1000/1875], Loss: 0.0863\n",
            "Epoch [10/15], Step [1000/1875], Loss: 0.0810\n",
            "Epoch [11/15], Step [1000/1875], Loss: 0.0745\n",
            "Epoch [12/15], Step [1000/1875], Loss: 0.0739\n",
            "Epoch [13/15], Step [1000/1875], Loss: 0.0683\n",
            "Epoch [14/15], Step [1000/1875], Loss: 0.0629\n",
            "Epoch [15/15], Step [1000/1875], Loss: 0.0604\n",
            "Accuracy on the test set: 96.63%\n",
            "==============================\n",
            "Running Test 3:\n",
            "Epoch [1/20], Step [1000/1875], Loss: 0.5003\n",
            "Epoch [2/20], Step [1000/1875], Loss: 0.2379\n"
          ]
        }
      ],
      "source": [
        "for idx, param_set in enumerate(params, start=1):\n",
        "    print(f\"Running Test {idx}:\")\n",
        "    \n",
        "    # Creazione dell'istanza del modello\n",
        "    input_size = 28 * 28\n",
        "    output_size = 10\n",
        "    model = NeuralNetwork(input_size, output_size, param_set[\"hidden_layers\"])\n",
        "\n",
        "    # Definizione del criterio di loss e dell'ottimizzatore\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=param_set[\"lr\"])\n",
        "\n",
        "    # Definizione del numero di epoche e del batch size\n",
        "    num_epochs = param_set[\"epochs\"]\n",
        "    batch_size = 32\n",
        "\n",
        "    # Creazione del DataLoader per il set di addestramento\n",
        "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # Training del modello\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        for i, (inputs, labels) in enumerate(train_loader, 0):\n",
        "            inputs = inputs.view(-1, 28*28)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            if (i+1) % 1000 == 0:\n",
        "                print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {running_loss/1000:.4f}')\n",
        "                running_loss = 0.0\n",
        "                \n",
        "    # Valutazione dell'accuratezza sul set di test\n",
        "    test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
        "    model.eval()\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs = inputs.view(-1, 28*28)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Accuracy on the test set: {accuracy:.2f}%')\n",
        "    print(\"=\" * 30)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
