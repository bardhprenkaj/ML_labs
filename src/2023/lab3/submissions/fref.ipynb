{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5AYt3Cn35zA"
      },
      "source": [
        "**Challenge: Implement a Multiclass Classification Neural Network using PyTorch**\n",
        "\n",
        "Objective:\n",
        "Build a feedforward neural network using PyTorch to predict the species of iris flowers in a multiclass classification problem. The dataset used for this challenge is the Iris dataset, which consists of features like sepal length, sepal width, petal length, and petal width.\n",
        "\n",
        "Steps:\n",
        "\n",
        "1. **Data Preparation**: Load the MNIST dataset using ```torchvision.datasets.MNIST```. Standardize/normalize the features. Split the dataset into training and testing sets using, for example, ```sklearn.model_selection.train_test_split()```. **Bonus scores**: *use PyTorch's built-* ```DataLoader``` *to split the dataset*.\n",
        "\n",
        "2. **Neural Network Architecture**: Define a simple feedforward neural network using PyTorch's ```nn.Module```. Design the input layer to match the number of features in the MNIST dataset and the output layer to have as many neurons as there are classes (10). You can experiment with the number of hidden layers and neurons to optimize the performance. **Bonus scores**: *Make your architecture flexibile to have as many hidden layers as the user wants, and use hyperparameter optimization to select the best number of hidden layeres.*\n",
        "\n",
        "3. **Loss Function and Optimizer**: Choose an appropriate loss function for multiclass classification. Select an optimizer, like SGD (Stochastic Gradient Descent) or Adam.\n",
        "\n",
        "4. **Training**: Write a training loop to iterate over the dataset.\n",
        "Forward pass the input through the network, calculate the loss, and perform backpropagation. Update the weights of the network using the chosen optimizer.\n",
        "\n",
        "5. **Testing**: Evaluate the trained model on the test set. Calculate the accuracy of the model.\n",
        "\n",
        "6. **Optimization**: Experiment with hyperparameters (learning rate, number of epochs, etc.) to optimize the model's performance. Consider adjusting the neural network architecture for better results. **Notice that you can't use the optimization algorithms from scikit-learn that we saw in lab1: e.g.,** ```GridSearchCV```.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. **Data Preparation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "5XLynrxJ33v6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# load MNIST dataset & transform to tensors\n",
        "data = torchvision.datasets.MNIST(root=\"./data\", train=True, download=True, transform=torchvision.transforms.ToTensor())\n",
        "og_features = data.data\n",
        "labels = data.targets\n",
        "\n",
        "# Standardize pixel values\n",
        "scaler = StandardScaler()\n",
        "standardized_features = scaler.fit_transform(og_features.view(og_features.size(0), -1)) # flatten for NN input\n",
        "\n",
        "# train/test split\n",
        "train_features, test_features, labels_train, labels_test = train_test_split(\n",
        "    standardized_features, labels, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# DataLoader for batches\n",
        "train_dataset = TensorDataset(torch.Tensor(train_features), torch.Tensor(labels_train))\n",
        "test_dataset = TensorDataset(torch.Tensor(test_features), torch.Tensor(labels_test))\n",
        "\n",
        "batch_size = 512 # will be optimized\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. **Neural Network Architecture**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MyNN(\n",
            "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
            "  (additional_hidden_layers): ModuleList()\n",
            "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "class MyNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_additional_hidden_layers):\n",
        "        super(MyNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.additional_hidden_layers = nn.ModuleList([nn.Linear(hidden_size, hidden_size) for _ in range(num_additional_hidden_layers)])\n",
        "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        for layer in self.additional_hidden_layers:\n",
        "            x = F.relu(layer(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "input_size = 28*28  # flattened image size\n",
        "output_size = 10  # 10 digits\n",
        "#we can change those two parameters to use different architectures\n",
        "hidden_size = 128 # optimize this at 6)\n",
        "num_additional_hidden_layers = 0 # we start with one hidden layer and optimize at 6)\n",
        "\n",
        "\n",
        "# Create model\n",
        "model = MyNN(input_size, hidden_size, output_size, num_additional_hidden_layers)\n",
        "\n",
        "# Print architecture\n",
        "print(model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3. **Loss Function and Optimizer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss Function: CrossEntropyLoss()\n",
            "Optimizer: Adam (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    capturable: False\n",
            "    differentiable: False\n",
            "    eps: 1e-08\n",
            "    foreach: None\n",
            "    fused: None\n",
            "    lr: 0.001\n",
            "    maximize: False\n",
            "    weight_decay: 0\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Print the loss function and optimizer\n",
        "print(\"Loss Function:\", loss_fn)\n",
        "print(\"Optimizer:\", optimizer)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4. **Training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Finished epoch 0, latest loss 0.3141784071922302\n",
            "Finished epoch 1, latest loss 0.17846040427684784\n",
            "Finished epoch 2, latest loss 0.13639381527900696\n",
            "Finished epoch 3, latest loss 0.06876114010810852\n",
            "Finished epoch 4, latest loss 0.10731310397386551\n",
            "Finished epoch 5, latest loss 0.09498702734708786\n",
            "Finished epoch 6, latest loss 0.055537376552820206\n",
            "Finished epoch 7, latest loss 0.05380956456065178\n",
            "Finished epoch 8, latest loss 0.03768095746636391\n",
            "Finished epoch 9, latest loss 0.04853275790810585\n"
          ]
        }
      ],
      "source": [
        "epochs = 10 \n",
        "batch_size = 512\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()  #train mode\n",
        "    \n",
        "    for inputs, labels in train_loader:\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        loss = loss_fn(outputs, labels)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Finished epoch {epoch}, latest loss {loss}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "5. **Testing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 0.9724\n"
          ]
        }
      ],
      "source": [
        "model.eval()  #evaluation mode\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader: # because we also use batches for testing\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = correct / total\n",
        "\n",
        "print(f'Test Accuracy: {accuracy:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "6. **Optimization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2023-12-20 21:52:00,160] A new study created in memory with name: no-name-ee9ac213-945f-4674-ad3a-41c030774137\n",
            "[I 2023-12-20 21:52:15,670] Trial 0 finished with value: 0.9744166666666667 and parameters: {'learning_rate': 0.001, 'num_epochs': 10, 'hidden_size': 128, 'num_additional_hidden_layers': 2, 'batch_size': 256}. Best is trial 0 with value: 0.9744166666666667.\n",
            "[I 2023-12-20 21:53:51,736] Trial 1 finished with value: 0.15391666666666667 and parameters: {'learning_rate': 0.1, 'num_epochs': 10, 'hidden_size': 1028, 'num_additional_hidden_layers': 1, 'batch_size': 256}. Best is trial 0 with value: 0.9744166666666667.\n",
            "[I 2023-12-20 21:54:12,922] Trial 2 finished with value: 0.18458333333333332 and parameters: {'learning_rate': 0.1, 'num_epochs': 10, 'hidden_size': 512, 'num_additional_hidden_layers': 1, 'batch_size': 1024}. Best is trial 0 with value: 0.9744166666666667.\n",
            "[I 2023-12-20 21:55:47,344] Trial 3 finished with value: 0.9783333333333334 and parameters: {'learning_rate': 0.001, 'num_epochs': 15, 'hidden_size': 1028, 'num_additional_hidden_layers': 2, 'batch_size': 1024}. Best is trial 3 with value: 0.9783333333333334.\n",
            "[I 2023-12-20 21:56:15,470] Trial 4 finished with value: 0.97325 and parameters: {'learning_rate': 0.001, 'num_epochs': 15, 'hidden_size': 256, 'num_additional_hidden_layers': 3, 'batch_size': 1024}. Best is trial 3 with value: 0.9783333333333334.\n",
            "[I 2023-12-20 21:57:11,434] Trial 5 finished with value: 0.098 and parameters: {'learning_rate': 0.1, 'num_epochs': 15, 'hidden_size': 256, 'num_additional_hidden_layers': 1, 'batch_size': 256}. Best is trial 3 with value: 0.9783333333333334.\n",
            "[I 2023-12-20 21:58:36,992] Trial 6 finished with value: 0.10158333333333333 and parameters: {'learning_rate': 0.1, 'num_epochs': 10, 'hidden_size': 512, 'num_additional_hidden_layers': 3, 'batch_size': 256}. Best is trial 3 with value: 0.9783333333333334.\n",
            "[I 2023-12-20 21:58:50,330] Trial 7 finished with value: 0.20125 and parameters: {'learning_rate': 0.1, 'num_epochs': 15, 'hidden_size': 128, 'num_additional_hidden_layers': 1, 'batch_size': 1024}. Best is trial 3 with value: 0.9783333333333334.\n",
            "[I 2023-12-20 22:00:31,558] Trial 8 finished with value: 0.9115833333333333 and parameters: {'learning_rate': 0.01, 'num_epochs': 10, 'hidden_size': 1028, 'num_additional_hidden_layers': 1, 'batch_size': 256}. Best is trial 3 with value: 0.9783333333333334.\n",
            "[I 2023-12-20 22:01:20,178] Trial 9 finished with value: 0.9741666666666666 and parameters: {'learning_rate': 0.001, 'num_epochs': 15, 'hidden_size': 512, 'num_additional_hidden_layers': 1, 'batch_size': 256}. Best is trial 3 with value: 0.9783333333333334.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Hyperparameters:\n",
            "Learning Rate: 0.001\n",
            "Number of Epochs: 15\n",
            "Hidden Size: 1028\n",
            "Number of Hidden Layers: 3\n"
          ]
        }
      ],
      "source": [
        "import optuna\n",
        "\n",
        "def objective(trial):\n",
        "    # hyperparameters to be optimized\n",
        "    learning_rate = trial.suggest_categorical('learning_rate', [0.001, 0.01, 0.1])\n",
        "    num_epochs = trial.suggest_categorical('num_epochs', [10, 15])\n",
        "    hidden_size = trial.suggest_categorical('hidden_size', [128, 256, 512, 1028]) # not optimal, but this will the same for all layers\n",
        "    num_additional_hidden_layers = trial.suggest_categorical('num_additional_hidden_layers', [1, 2, 3])\n",
        "    batch_size = trial.suggest_categorical('batch_size', [256, 512, 1024])\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True) # for batch size \n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False) # for batch size \n",
        "    model = MyNN(input_size, hidden_size, output_size, num_additional_hidden_layers) # for hidden size and additional layers\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate) # for learning rate\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        for inputs, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = loss_fn(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    # evaluation\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = correct / total\n",
        "\n",
        "    return accuracy\n",
        "\n",
        "# Create a study object and optimize\n",
        "study = optuna.create_study(direction='maximize')\n",
        "study.optimize(objective, n_trials=10)\n",
        "\n",
        "# Get the best hyperparameters\n",
        "best_params = study.best_params\n",
        "best_learning_rate = best_params['learning_rate']\n",
        "best_num_epochs = best_params['num_epochs']\n",
        "best_hidden_size = best_params['hidden_size']\n",
        "best_num_additional_hidden_layers = best_params['num_additional_hidden_layers']\n",
        "best_batch_size = best_params['batch_size']\n",
        "\n",
        "\n",
        "print(\"Best Hyperparameters:\")\n",
        "print(f\"Learning Rate: {best_learning_rate}\")\n",
        "print(f\"Number of Epochs: {best_num_epochs}\")\n",
        "print(f\"Hidden Size: {best_hidden_size}\")\n",
        "print(f\"Number of Hidden Layers: {best_num_additional_hidden_layers+1}\") # +1 because we started already with one hidden layer\n",
        "print(f\"Batch Size: {best_batch_size}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "That took about 9 minutes for me.   \n",
        "\n",
        "Finally the best model has an accuracy of 0.9783333333333334, which is slightly better than my initial model.   \n",
        "I guess my inital model was already pretty good, because i manually tested learning rate, batch size and epoch number before.  \n",
        "\n",
        "Hyperparameters:  \n",
        "Learning Rate: 0.001  \n",
        "Number of Epochs: 15  \n",
        "Hidden Size: 1028  \n",
        "Number of Hidden Layers: 3  \n",
        "Batch Size: 1024\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
