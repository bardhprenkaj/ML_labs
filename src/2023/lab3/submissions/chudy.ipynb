{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5AYt3Cn35zA"
      },
      "source": [
        "**Challenge: Implement a Multiclass Classification Neural Network using PyTorch**\n",
        "\n",
        "Objective:\n",
        "Build a neural network using PyTorch to predict handwritten digits of MNIST.\n",
        "\n",
        "Steps:\n",
        "\n",
        "1. **Data Preparation**: Load the MNIST dataset using ```torchvision.datasets.MNIST```. Standardize/normalize the features. Split the dataset into training and testing sets using, for example, ```sklearn.model_selection.train_test_split()```. **Bonus scores**: *use PyTorch's built-* ```DataLoader``` *to split the dataset*.\n",
        "\n",
        "2. **Neural Network Architecture**: Define a simple feedforward neural network using PyTorch's ```nn.Module```. Design the input layer to match the number of features in the MNIST dataset and the output layer to have as many neurons as there are classes (10). You can experiment with the number of hidden layers and neurons to optimize the performance. **Bonus scores**: *Make your architecture flexibile to have as many hidden layers as the user wants, and use hyperparameter optimization to select the best number of hidden layeres.*\n",
        "\n",
        "3. **Loss Function and Optimizer**: Choose an appropriate loss function for multiclass classification. Select an optimizer, like SGD (Stochastic Gradient Descent) or Adam.\n",
        "\n",
        "4. **Training**: Write a training loop to iterate over the dataset.\n",
        "Forward pass the input through the network, calculate the loss, and perform backpropagation. Update the weights of the network using the chosen optimizer.\n",
        "\n",
        "5. **Testing**: Evaluate the trained model on the test set. Calculate the accuracy of the model.\n",
        "\n",
        "6. **Optimization**: Experiment with hyperparameters (learning rate, number of epochs, etc.) to optimize the model's performance. Consider adjusting the neural network architecture for better results. **Notice that you can't use the optimization algorithms from scikit-learn that we saw in lab1: e.g.,** ```GridSearchCV```.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "5XLynrxJ33v6"
      },
      "outputs": [],
      "source": [
        "# insert code here\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_data():\n",
        "    transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
        "\n",
        "    # get the training and test set of MNIST\n",
        "    trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "    testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "    return trainset, testset\n",
        "\n",
        "# get the training and test data\n",
        "trainset, testset = load_data()\n",
        "\n",
        "batch_size = 32\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MnistModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv2d(1, 16, kernel_size=(5,5), stride=1, padding=0)\n",
        "    self.act1 = nn.ReLU()\n",
        "    self.pool1 = nn.MaxPool2d(kernel_size=(2, 2))\n",
        "    self.drop1 = nn.Dropout(0.3)\n",
        "\n",
        "    self.conv2 = nn.Conv2d(16, 32, kernel_size=(5,5), stride=1, padding=0)\n",
        "    self.act2 = nn.ReLU()\n",
        "    self.pool2 = nn.MaxPool2d(kernel_size=(2, 2))\n",
        "    self.drop2 = nn.Dropout(0.3)\n",
        "\n",
        "\n",
        "    self.flat = nn.Flatten()\n",
        "\n",
        "    self.fc3 = nn.Linear(512, 128)\n",
        "    self.act3 = nn.ReLU()\n",
        "    self.drop3 = nn.Dropout(0.5)\n",
        "\n",
        "    self.fc4 = nn.Linear(128, 10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # input 1x28x28, output 16x24x24\n",
        "    x = self.act1(self.conv1(x))\n",
        "    # input 16x24x24 output 16x12x12\n",
        "    x = self.pool1(x)\n",
        "    x = self.drop1(x)\n",
        "    # input 16x12x12, output 32x8x8\n",
        "    x = self.act2(self.conv2(x))\n",
        "    # input 32x8x8, output 32x4x4\n",
        "    x = self.pool2(x)\n",
        "    x = self.drop2(x)\n",
        "    # input 32x4x4, output 512\n",
        "    x = self.flat(x)\n",
        "    # input 512, output 128\n",
        "    x = self.act3(self.fc3(x))\n",
        "    x = self.drop3(x)\n",
        "    # input 128, output 10\n",
        "    x = self.fc4(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 --> loss = 1.1014271545171737\n",
            "Epoch 1: model accuracy 86.70%\n",
            "Epoch 2 --> loss = 0.3414322280963262\n",
            "Epoch 2: model accuracy 91.98%\n",
            "Epoch 3 --> loss = 0.24576735883752504\n",
            "Epoch 3: model accuracy 93.67%\n",
            "Epoch 4 --> loss = 0.20111183825234571\n",
            "Epoch 4: model accuracy 94.96%\n",
            "Epoch 5 --> loss = 0.17453749780307212\n",
            "Epoch 5: model accuracy 95.36%\n",
            "Epoch 6 --> loss = 0.15684192607154449\n",
            "Epoch 6: model accuracy 95.99%\n",
            "Epoch 7 --> loss = 0.1434239830593268\n",
            "Epoch 7: model accuracy 96.05%\n",
            "Epoch 8 --> loss = 0.13106127521197\n",
            "Epoch 8: model accuracy 96.38%\n",
            "Epoch 9 --> loss = 0.1219132648701469\n",
            "Epoch 9: model accuracy 96.63%\n",
            "Epoch 10 --> loss = 0.11871466893802086\n",
            "Epoch 10: model accuracy 96.64%\n",
            "Epoch 11 --> loss = 0.11139947862563034\n",
            "Epoch 11: model accuracy 97.22%\n",
            "Epoch 12 --> loss = 0.10683415232989937\n",
            "Epoch 12: model accuracy 97.20%\n",
            "Epoch 13 --> loss = 0.10037438258503874\n",
            "Epoch 13: model accuracy 97.07%\n",
            "Epoch 14 --> loss = 0.09608800989923377\n",
            "Epoch 14: model accuracy 97.06%\n",
            "Epoch 15 --> loss = 0.09397796082037191\n",
            "Epoch 15: model accuracy 97.28%\n",
            "Epoch 16 --> loss = 0.08910965760014951\n",
            "Epoch 16: model accuracy 97.59%\n",
            "Epoch 17 --> loss = 0.08697561425069968\n",
            "Epoch 17: model accuracy 97.49%\n",
            "Epoch 18 --> loss = 0.08418358590180675\n",
            "Epoch 18: model accuracy 97.50%\n",
            "Epoch 19 --> loss = 0.08242950593742232\n",
            "Epoch 19: model accuracy 97.70%\n",
            "Epoch 20 --> loss = 0.07926631523780525\n",
            "Epoch 20: model accuracy 97.77%\n",
            "Epoch 21 --> loss = 0.07770258808576812\n",
            "Epoch 21: model accuracy 97.71%\n",
            "Epoch 22 --> loss = 0.07345916738652934\n",
            "Epoch 22: model accuracy 97.55%\n",
            "Epoch 23 --> loss = 0.07334128478169441\n",
            "Epoch 23: model accuracy 97.86%\n",
            "Epoch 24 --> loss = 0.07375612619323656\n",
            "Epoch 24: model accuracy 97.87%\n",
            "Epoch 25 --> loss = 0.07214880620936552\n",
            "Epoch 25: model accuracy 97.85%\n",
            "Epoch 26 --> loss = 0.0694134283779189\n",
            "Epoch 26: model accuracy 97.97%\n",
            "Epoch 27 --> loss = 0.06868811397807052\n",
            "Epoch 27: model accuracy 97.98%\n",
            "Epoch 28 --> loss = 0.06480101302598293\n",
            "Epoch 28: model accuracy 97.94%\n",
            "Epoch 29 --> loss = 0.06518246259450292\n",
            "Epoch 29: model accuracy 97.95%\n",
            "Epoch 30 --> loss = 0.06409884929911544\n",
            "Epoch 30: model accuracy 98.01%\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "model = MnistModel()\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "n_epochs = 30\n",
        "for epoch in range(n_epochs):\n",
        "  losses = []\n",
        "  for inputs, labels in trainloader:\n",
        "    # forward, backward, and then weight update\n",
        "    y_pred = model(inputs)\n",
        "    loss = loss_fn(y_pred, labels)\n",
        "    losses.append(loss.item())\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  print(f'Epoch {epoch + 1} --> loss = {np.mean(losses)}')\n",
        "\n",
        "  acc = 0\n",
        "  count = 0\n",
        "  for inputs, labels in testloader:\n",
        "    y_pred = model(inputs)\n",
        "    acc += (torch.argmax(y_pred, 1) == labels).float().sum()\n",
        "    count += len(labels)\n",
        "  acc /= count\n",
        "  print(\"Epoch %d: model accuracy %.2f%%\" % (epoch+1, acc*100))\n",
        "\n",
        "torch.save(model.state_dict(), \"Mnistmodel.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# I tried to use Ray Tune for hyperparameter optimization as suggested by pytorch team on the website: https://pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html\n",
        "# but unfortunately didn't manage to use it correctly and ran into the error which I couldn't overcome\n",
        "import os\n",
        "import tempfile\n",
        "from ray import train, tune\n",
        "from ray.tune.schedulers import ASHAScheduler\n",
        "\n",
        "def train_mnist(config):\n",
        "    model = MnistModel()\n",
        "\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=float(config[\"lr\"]), momentum=0.9)\n",
        "\n",
        "    checkpoint = train.get_checkpoint()\n",
        "\n",
        "    if checkpoint:\n",
        "        with checkpoint.as_directory() as checkpoint_dir:\n",
        "            checkpoint_dict = torch.load(os.path.join(checkpoint_dir, \"checkpoint.pt\"))\n",
        "        start_epoch = checkpoint_dict[\"epoch\"]\n",
        "        model.load_state_dict(checkpoint_dict[\"model_state_dict\"])\n",
        "        optimizer.load_state_dict(checkpoint_dict[\"optimizer_state_dict\"])\n",
        "    else:\n",
        "        start_epoch = 0\n",
        "\n",
        "    trainset, testset = load_data()\n",
        "\n",
        "    trainloader = torch.utils.data.DataLoader(\n",
        "        trainset, batch_size=int(config[\"batch_size\"]), shuffle=True)\n",
        "    testloader = torch.utils.data.DataLoader(\n",
        "        testset, batch_size=int(config[\"batch_size\"]), shuffle=True)\n",
        "    \n",
        "    for epoch in range(start_epoch, 30):\n",
        "        losses = []\n",
        "        for inputs, labels in trainloader:\n",
        "            # forward, backward, and then weight update\n",
        "            y_pred = model(inputs)\n",
        "            loss = loss_fn(y_pred, labels)\n",
        "            losses.append(loss.item())\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        print(f'Epoch {epoch + 1} --> loss = {np.mean(losses)}')\n",
        "\n",
        "        acc = 0\n",
        "        count = 0\n",
        "        for inputs, labels in testloader:\n",
        "            y_pred = model(inputs)\n",
        "            acc += (torch.argmax(y_pred, 1) == labels).float().sum()\n",
        "            count += len(labels)\n",
        "        acc /= count\n",
        "        print(\"Epoch %d: model accuracy %.2f%%\" % (epoch+1, acc*100))\n",
        "        metrics = {\"loss\": np.mean(losses), \"accuracy\": acc*100}\n",
        "        with tempfile.TemporaryDirectory() as tempdir:\n",
        "            torch.save(\n",
        "                {\"epoch\": epoch,\n",
        "                \"model_state_dict\": model.state_dict(),\n",
        "                \"optimizer_state_dict\": optimizer.state_dict()},\n",
        "                os.path.join(tempdir, \"checkpoint.pt\"),\n",
        "            )\n",
        "            train.report(\n",
        "                metrics=metrics,\n",
        "                checkpoint=train.Checkpoint.from_directory(tempdir)\n",
        "            )\n",
        "    print(\"Finished Training\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div class=\"tuneStatus\">\n",
              "  <div style=\"display: flex;flex-direction: row\">\n",
              "    <div style=\"display: flex;flex-direction: column;\">\n",
              "      <h3>Tune Status</h3>\n",
              "      <table>\n",
              "<tbody>\n",
              "<tr><td>Current time:</td><td>2023-12-21 14:40:40</td></tr>\n",
              "<tr><td>Running for: </td><td>00:00:47.10        </td></tr>\n",
              "<tr><td>Memory:      </td><td>9.0/15.3 GiB       </td></tr>\n",
              "</tbody>\n",
              "</table>\n",
              "    </div>\n",
              "    <div class=\"vDivider\"></div>\n",
              "    <div class=\"systemInfo\">\n",
              "      <h3>System Info</h3>\n",
              "      Using FIFO scheduling algorithm.<br>Logical resource usage: 1.0/8 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
              "    </div>\n",
              "    <div class=\"vDivider\"></div>\n",
              "<div class=\"messages\">\n",
              "  <h3>Messages</h3>\n",
              "  \n",
              "  \n",
              "  Number of errored trials: 1<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name             </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                                                             </th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>train_mnist_6bda9_00000</td><td style=\"text-align: right;\">           1</td><td>/home/szymon/ray_results/train_mnist_2023-12-21_14-39-53/train_mnist_6bda9_00000_0_batch_size=8,lr=0.0008_2023-12-21_14-39-53/error.txt</td></tr>\n",
              "</tbody>\n",
              "</table>\n",
              "</div>\n",
              "<style>\n",
              ".messages {\n",
              "  color: var(--jp-ui-font-color1);\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "  padding-left: 1em;\n",
              "  overflow-y: auto;\n",
              "}\n",
              ".messages h3 {\n",
              "  font-weight: bold;\n",
              "}\n",
              ".vDivider {\n",
              "  border-left-width: var(--jp-border-width);\n",
              "  border-left-color: var(--jp-border-color0);\n",
              "  border-left-style: solid;\n",
              "  margin: 0.5em 1em 0.5em 1em;\n",
              "}\n",
              "</style>\n",
              "\n",
              "  </div>\n",
              "  <div class=\"hDivider\"></div>\n",
              "  <div class=\"trialStatus\">\n",
              "    <h3>Trial Status</h3>\n",
              "    <table>\n",
              "<thead>\n",
              "<tr><th>Trial name             </th><th>status  </th><th>loc                  </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">         lr</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>train_mnist_6bda9_00000</td><td>ERROR   </td><td>192.168.169.223:11756</td><td style=\"text-align: right;\">           8</td><td style=\"text-align: right;\">0.000804061</td></tr>\n",
              "</tbody>\n",
              "</table>\n",
              "  </div>\n",
              "</div>\n",
              "<style>\n",
              ".tuneStatus {\n",
              "  color: var(--jp-ui-font-color1);\n",
              "}\n",
              ".tuneStatus .systemInfo {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "}\n",
              ".tuneStatus td {\n",
              "  white-space: nowrap;\n",
              "}\n",
              ".tuneStatus .trialStatus {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "}\n",
              ".tuneStatus h3 {\n",
              "  font-weight: bold;\n",
              "}\n",
              ".tuneStatus .hDivider {\n",
              "  border-bottom-width: var(--jp-border-width);\n",
              "  border-bottom-color: var(--jp-border-color0);\n",
              "  border-bottom-style: solid;\n",
              "}\n",
              ".tuneStatus .vDivider {\n",
              "  border-left-width: var(--jp-border-width);\n",
              "  border-left-color: var(--jp-border-color0);\n",
              "  border-left-style: solid;\n",
              "  margin: 0.5em 1em 0.5em 1em;\n",
              "}\n",
              "</style>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(train_mnist pid=11756)\u001b[0m Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "\u001b[36m(train_mnist pid=11756)\u001b[0m Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/9912422 [00:00<?, ?it/s]\n",
            "  1%|          | 65536/9912422 [00:00<00:32, 299521.32it/s]\n",
            "  2%|▏         | 163840/9912422 [00:00<00:21, 460706.38it/s]\n",
            "  2%|▏         | 229376/9912422 [00:00<00:28, 340699.02it/s]\n",
            "  3%|▎         | 327680/9912422 [00:00<00:20, 460045.41it/s]\n",
            "  4%|▍         | 393216/9912422 [00:00<00:24, 393187.99it/s]\n",
            "  5%|▍         | 491520/9912422 [00:01<00:20, 462899.35it/s]\n",
            "  6%|▋         | 622592/9912422 [00:01<00:14, 633448.29it/s]\n",
            "  8%|▊         | 753664/9912422 [00:01<00:12, 761883.09it/s]\n",
            "  9%|▉         | 884736/9912422 [00:01<00:10, 840199.39it/s]\n",
            " 11%|█         | 1048576/9912422 [00:01<00:08, 988017.27it/s]\n",
            " 12%|█▏        | 1179648/9912422 [00:02<00:14, 605906.03it/s]\n",
            " 13%|█▎        | 1277952/9912422 [00:02<00:13, 663819.73it/s]\n",
            " 14%|█▍        | 1376256/9912422 [00:02<00:12, 693449.10it/s]\n",
            " 15%|█▌        | 1507328/9912422 [00:02<00:12, 699402.60it/s]\n",
            " 18%|█▊        | 1736704/9912422 [00:02<00:08, 1015882.78it/s]\n",
            " 19%|█▉        | 1867776/9912422 [00:02<00:11, 728208.63it/s] \n",
            " 20%|██        | 2031616/9912422 [00:03<00:11, 677182.48it/s]\n",
            " 23%|██▎       | 2293760/9912422 [00:03<00:08, 944547.10it/s]\n",
            " 25%|██▌       | 2523136/9912422 [00:03<00:06, 1184786.84it/s]\n",
            " 28%|██▊       | 2785280/9912422 [00:03<00:05, 1418818.97it/s]\n",
            " 30%|███       | 2981888/9912422 [00:03<00:05, 1308456.07it/s]\n",
            " 32%|███▏      | 3145728/9912422 [00:03<00:05, 1200050.00it/s]\n",
            " 33%|███▎      | 3309568/9912422 [00:04<00:06, 962140.98it/s] \n",
            " 35%|███▍      | 3440640/9912422 [00:04<00:07, 810355.66it/s]\n",
            " 36%|███▌      | 3538944/9912422 [00:04<00:08, 760466.72it/s]\n",
            " 37%|███▋      | 3702784/9912422 [00:04<00:06, 914854.48it/s]\n",
            " 39%|███▊      | 3833856/9912422 [00:04<00:06, 942267.93it/s]\n",
            " 40%|███▉      | 3964928/9912422 [00:04<00:06, 973785.54it/s]\n",
            " 41%|████▏     | 4096000/9912422 [00:05<00:06, 921648.78it/s]\n",
            " 43%|████▎     | 4227072/9912422 [00:05<00:06, 887421.80it/s]\n",
            " 44%|████▍     | 4390912/9912422 [00:05<00:05, 927713.38it/s]\n",
            " 45%|████▌     | 4489216/9912422 [00:05<00:08, 635149.05it/s]\n",
            " 46%|████▋     | 4587520/9912422 [00:06<00:11, 474267.29it/s]\n",
            " 48%|████▊     | 4751360/9912422 [00:06<00:08, 639363.38it/s]\n",
            " 49%|████▉     | 4849664/9912422 [00:06<00:08, 591501.22it/s]\n",
            " 50%|█████     | 4980736/9912422 [00:06<00:07, 678997.94it/s]\n",
            " 52%|█████▏    | 5111808/9912422 [00:06<00:07, 676688.20it/s]\n",
            " 54%|█████▍    | 5341184/9912422 [00:06<00:05, 808267.70it/s]\n",
            " 55%|█████▍    | 5439488/9912422 [00:07<00:05, 756074.14it/s]\n",
            " 56%|█████▌    | 5537792/9912422 [00:07<00:05, 778709.29it/s]\n",
            " 57%|█████▋    | 5668864/9912422 [00:07<00:04, 859396.49it/s]\n",
            " 60%|█████▉    | 5898240/9912422 [00:07<00:03, 1058010.99it/s]\n",
            " 61%|██████    | 6029312/9912422 [00:07<00:03, 1052206.81it/s]\n",
            " 62%|██████▏   | 6160384/9912422 [00:07<00:03, 1083197.48it/s]\n",
            " 65%|██████▌   | 6455296/9912422 [00:07<00:02, 1507905.08it/s]\n",
            " 67%|██████▋   | 6619136/9912422 [00:07<00:02, 1526225.29it/s]\n",
            " 70%|██████▉   | 6914048/9912422 [00:07<00:01, 1842791.34it/s]\n",
            " 72%|███████▏  | 7143424/9912422 [00:08<00:01, 1642592.38it/s]\n",
            " 74%|███████▍  | 7340032/9912422 [00:08<00:01, 1474119.16it/s]\n",
            " 76%|███████▌  | 7503872/9912422 [00:08<00:01, 1250430.52it/s]\n",
            " 77%|███████▋  | 7667712/9912422 [00:08<00:02, 1072604.28it/s]\n",
            " 80%|███████▉  | 7897088/9912422 [00:08<00:01, 1195643.01it/s]\n",
            " 82%|████████▏ | 8093696/9912422 [00:09<00:01, 1110369.75it/s]\n",
            " 86%|████████▋ | 8552448/9912422 [00:09<00:00, 1772809.78it/s]\n",
            " 89%|████████▉ | 8847360/9912422 [00:09<00:00, 2024801.61it/s]\n",
            " 92%|█████████▏| 9109504/9912422 [00:09<00:00, 1254005.58it/s]\n",
            " 96%|█████████▌| 9469952/9912422 [00:09<00:00, 1570253.79it/s]\n",
            " 98%|█████████▊| 9699328/9912422 [00:09<00:00, 1700222.34it/s]\n",
            "100%|██████████| 9912422/9912422 [00:09<00:00, 992231.52it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(train_mnist pid=11756)\u001b[0m Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\u001b[36m(train_mnist pid=11756)\u001b[0m \n",
            "\u001b[36m(train_mnist pid=11756)\u001b[0m Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "\u001b[36m(train_mnist pid=11756)\u001b[0m Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/28881 [00:00<?, ?it/s]\n",
            "100%|██████████| 28881/28881 [00:00<00:00, 372763.06it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(train_mnist pid=11756)\u001b[0m Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\u001b[36m(train_mnist pid=11756)\u001b[0m \n",
            "\u001b[36m(train_mnist pid=11756)\u001b[0m Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "\u001b[36m(train_mnist pid=11756)\u001b[0m Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/1648877 [00:00<?, ?it/s]\n",
            "  2%|▏         | 32768/1648877 [00:00<00:05, 310995.00it/s]\n",
            "  4%|▍         | 65536/1648877 [00:00<00:06, 228264.19it/s]\n",
            " 10%|▉         | 163840/1648877 [00:00<00:03, 438226.73it/s]\n",
            " 14%|█▍        | 229376/1648877 [00:00<00:02, 504727.24it/s]\n",
            " 18%|█▊        | 294912/1648877 [00:00<00:04, 300121.57it/s]\n",
            " 24%|██▍       | 393216/1648877 [00:01<00:03, 400206.33it/s]\n",
            " 28%|██▊       | 458752/1648877 [00:01<00:02, 405807.26it/s]\n",
            " 32%|███▏      | 524288/1648877 [00:01<00:02, 449043.83it/s]\n",
            " 38%|███▊      | 622592/1648877 [00:01<00:01, 539444.92it/s]\n",
            " 46%|████▌     | 753664/1648877 [00:01<00:01, 680788.84it/s]\n",
            " 58%|█████▊    | 950272/1648877 [00:01<00:00, 946009.60it/s]\n",
            " 72%|███████▏  | 1179648/1648877 [00:01<00:00, 1242782.26it/s]\n",
            " 83%|████████▎ | 1376256/1648877 [00:02<00:00, 930608.69it/s] \n",
            "100%|██████████| 1648877/1648877 [00:02<00:00, 740925.36it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(train_mnist pid=11756)\u001b[0m Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\u001b[36m(train_mnist pid=11756)\u001b[0m \n",
            "\u001b[36m(train_mnist pid=11756)\u001b[0m Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "\u001b[36m(train_mnist pid=11756)\u001b[0m Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 50666299.91it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(train_mnist pid=11756)\u001b[0m Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\u001b[36m(train_mnist pid=11756)\u001b[0m \n",
            "\u001b[36m(train_mnist pid=11756)\u001b[0m Epoch 1 --> loss = 0.5652521514050352\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-12-21 14:40:40,584\tERROR tune_controller.py:1383 -- Trial task failed for trial train_mnist_6bda9_00000\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/szymon/.local/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
            "    result = ray.get(future)\n",
            "  File \"/home/szymon/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/home/szymon/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/szymon/.local/lib/python3.10/site-packages/ray/_private/worker.py\", line 2563, in get\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(ValueError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=11756, ip=192.168.169.223, actor_id=adbf5b7ed6bd2b1201df18e401000000, repr=train_mnist)\n",
            "  File \"/home/szymon/.local/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 342, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/home/szymon/.local/lib/python3.10/site-packages/ray/air/_internal/util.py\", line 91, in run\n",
            "    self._ret = self._target(*self._args, **self._kwargs)\n",
            "  File \"/home/szymon/.local/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 115, in <lambda>\n",
            "    training_func=lambda: self._trainable_func(self.config),\n",
            "  File \"/home/szymon/.local/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 332, in _trainable_func\n",
            "    output = fn()\n",
            "  File \"/tmp/ipykernel_9533/2979336149.py\", line 60, in train_mnist\n",
            "  File \"/home/szymon/.local/lib/python3.10/site-packages/ray/train/_internal/session.py\", line 644, in wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/home/szymon/.local/lib/python3.10/site-packages/ray/train/_internal/session.py\", line 706, in report\n",
            "    _get_session().report(metrics, checkpoint=checkpoint)\n",
            "  File \"/home/szymon/.local/lib/python3.10/site-packages/ray/train/_internal/session.py\", line 399, in report\n",
            "    raise ValueError(\n",
            "ValueError: Passing objects containg Torch tensors as metrics is not supported as it will throw an exception on deserialization. You can either convert the tensors to Python objects or report a `train.Checkpoint` with `ray.train.report` to store your Torch objects.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(train_mnist pid=11756)\u001b[0m Epoch 1: model accuracy 93.57%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-12-21 14:40:40,591\tERROR tune.py:1043 -- Trials did not complete: [train_mnist_6bda9_00000]\n",
            "2023-12-21 14:40:40,592\tINFO tune.py:1047 -- Total run time: 47.12 seconds (47.10 seconds for the tuning loop).\n"
          ]
        }
      ],
      "source": [
        "config = {\n",
        "\"lr\": tune.loguniform(1e-4, 1e-1),\n",
        "\"batch_size\": tune.choice([2, 4, 8, 16, 32])\n",
        "}\n",
        "\n",
        "tuner = tune.Tuner(train_mnist, param_space={\"lr\": tune.loguniform(1e-4, 1e-1), \"batch_size\": tune.choice([2, 4, 8, 16, 32])})\n",
        "result_grid = tuner.fit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Therefore I implemented a simple search for the best learning rate and batch size\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def train_mnist(n_epochs, lr, batch_size):\n",
        "    model = MnistModel()\n",
        "\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "    torch.manual_seed(42)\n",
        "\n",
        "\n",
        "    trainset, testset = load_data()\n",
        "\n",
        "    trainloader = torch.utils.data.DataLoader(\n",
        "        trainset, batch_size=batch_size, shuffle=True)\n",
        "    testloader = torch.utils.data.DataLoader(\n",
        "        testset, batch_size=batch_size, shuffle=True)\n",
        "    \n",
        "    for epoch in range(n_epochs):\n",
        "        losses = []\n",
        "        for inputs, labels in trainloader:\n",
        "            # forward, backward, and then weight update\n",
        "            y_pred = model(inputs)\n",
        "            loss = loss_fn(y_pred, labels)\n",
        "            losses.append(loss.item())\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        print(f'Epoch {epoch + 1} --> loss = {np.mean(losses)}')\n",
        "\n",
        "        acc = 0\n",
        "        count = 0\n",
        "        for inputs, labels in testloader:\n",
        "            y_pred = model(inputs)\n",
        "            acc += (torch.argmax(y_pred, 1) == labels).float().sum()\n",
        "            count += len(labels)\n",
        "        acc /= count\n",
        "        print(\"Epoch %d: model accuracy %.2f%%\" % (epoch+1, acc*100))\n",
        "    print(\"Finished Training\")\n",
        "    return {\"lr\": lr, \"batch_size\": batch_size, \"accuracy\": acc*100}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 --> loss = 1.0345506687482198\n",
            "Epoch 1: model accuracy 88.56%\n",
            "Epoch 2 --> loss = 0.3172469078083833\n",
            "Epoch 2: model accuracy 92.52%\n",
            "Epoch 3 --> loss = 0.23005055907169977\n",
            "Epoch 3: model accuracy 93.75%\n",
            "Epoch 4 --> loss = 0.19536977801024913\n",
            "Epoch 4: model accuracy 94.82%\n",
            "Epoch 5 --> loss = 0.1703737028516829\n",
            "Epoch 5: model accuracy 95.44%\n",
            "Epoch 6 --> loss = 0.1544893087312579\n",
            "Epoch 6: model accuracy 95.52%\n",
            "Epoch 7 --> loss = 0.1433250789185365\n",
            "Epoch 7: model accuracy 96.02%\n",
            "Epoch 8 --> loss = 0.1314623482055962\n",
            "Epoch 8: model accuracy 96.26%\n",
            "Epoch 9 --> loss = 0.12311351753920317\n",
            "Epoch 9: model accuracy 96.62%\n",
            "Epoch 10 --> loss = 0.11994809426863988\n",
            "Epoch 10: model accuracy 96.68%\n",
            "Epoch 11 --> loss = 0.11243394967572143\n",
            "Epoch 11: model accuracy 96.84%\n",
            "Epoch 12 --> loss = 0.10849212315777938\n",
            "Epoch 12: model accuracy 97.15%\n",
            "Epoch 13 --> loss = 0.1035498722984145\n",
            "Epoch 13: model accuracy 97.05%\n",
            "Epoch 14 --> loss = 0.0984101459559674\n",
            "Epoch 14: model accuracy 97.04%\n",
            "Epoch 15 --> loss = 0.0946028557681789\n",
            "Epoch 15: model accuracy 97.35%\n",
            "Finished Training\n",
            "Epoch 1 --> loss = 0.723402028008302\n",
            "Epoch 1: model accuracy 91.00%\n",
            "Epoch 2 --> loss = 0.23600526759463053\n",
            "Epoch 2: model accuracy 94.56%\n",
            "Epoch 3 --> loss = 0.17958052341407166\n",
            "Epoch 3: model accuracy 95.78%\n",
            "Epoch 4 --> loss = 0.1509732938764617\n",
            "Epoch 4: model accuracy 96.20%\n",
            "Epoch 5 --> loss = 0.13051255774038534\n",
            "Epoch 5: model accuracy 96.35%\n",
            "Epoch 6 --> loss = 0.11883773587470253\n",
            "Epoch 6: model accuracy 96.83%\n",
            "Epoch 7 --> loss = 0.11012008148812844\n",
            "Epoch 7: model accuracy 96.76%\n",
            "Epoch 8 --> loss = 0.1018283613938683\n",
            "Epoch 8: model accuracy 97.25%\n",
            "Epoch 9 --> loss = 0.09661829469802324\n",
            "Epoch 9: model accuracy 97.15%\n",
            "Epoch 10 --> loss = 0.0902617913761312\n",
            "Epoch 10: model accuracy 97.53%\n",
            "Epoch 11 --> loss = 0.08799429868012666\n",
            "Epoch 11: model accuracy 97.64%\n",
            "Epoch 12 --> loss = 0.08366360658283035\n",
            "Epoch 12: model accuracy 97.47%\n",
            "Epoch 13 --> loss = 0.07754161213506013\n",
            "Epoch 13: model accuracy 97.74%\n",
            "Epoch 14 --> loss = 0.07633631409589434\n",
            "Epoch 14: model accuracy 97.82%\n",
            "Epoch 15 --> loss = 0.07293898841465901\n",
            "Epoch 15: model accuracy 97.87%\n",
            "Finished Training\n",
            "Epoch 1 --> loss = 0.46237864533811807\n",
            "Epoch 1: model accuracy 94.30%\n",
            "Epoch 2 --> loss = 0.1588263808824122\n",
            "Epoch 2: model accuracy 96.25%\n",
            "Epoch 3 --> loss = 0.12099170893666644\n",
            "Epoch 3: model accuracy 96.45%\n",
            "Epoch 4 --> loss = 0.10067775643089165\n",
            "Epoch 4: model accuracy 97.21%\n",
            "Epoch 5 --> loss = 0.08704281501136721\n",
            "Epoch 5: model accuracy 97.61%\n",
            "Epoch 6 --> loss = 0.08061579864351079\n",
            "Epoch 6: model accuracy 97.73%\n",
            "Epoch 7 --> loss = 0.074183017285758\n",
            "Epoch 7: model accuracy 97.93%\n",
            "Epoch 8 --> loss = 0.07161964644764861\n",
            "Epoch 8: model accuracy 97.96%\n",
            "Epoch 9 --> loss = 0.06600910995975137\n",
            "Epoch 9: model accuracy 98.08%\n",
            "Epoch 10 --> loss = 0.0643606808311306\n",
            "Epoch 10: model accuracy 98.26%\n",
            "Epoch 11 --> loss = 0.06002679011481814\n",
            "Epoch 11: model accuracy 98.40%\n",
            "Epoch 12 --> loss = 0.05931512643934305\n",
            "Epoch 12: model accuracy 98.16%\n",
            "Epoch 13 --> loss = 0.058372019123705106\n",
            "Epoch 13: model accuracy 98.35%\n",
            "Epoch 14 --> loss = 0.05431689778744864\n",
            "Epoch 14: model accuracy 98.47%\n",
            "Epoch 15 --> loss = 0.05271538120292438\n",
            "Epoch 15: model accuracy 98.30%\n",
            "Finished Training\n",
            "Epoch 1 --> loss = 0.3549527539705858\n",
            "Epoch 1: model accuracy 94.95%\n",
            "Epoch 2 --> loss = 0.12921497846889932\n",
            "Epoch 2: model accuracy 96.73%\n",
            "Epoch 3 --> loss = 0.10248421988935442\n",
            "Epoch 3: model accuracy 97.59%\n",
            "Epoch 4 --> loss = 0.0895282221093153\n",
            "Epoch 4: model accuracy 97.56%\n",
            "Epoch 5 --> loss = 0.07825400768895828\n",
            "Epoch 5: model accuracy 97.85%\n",
            "Epoch 6 --> loss = 0.07304232336932716\n",
            "Epoch 6: model accuracy 98.01%\n",
            "Epoch 7 --> loss = 0.07068714752638673\n",
            "Epoch 7: model accuracy 98.00%\n",
            "Epoch 8 --> loss = 0.06149093235719641\n",
            "Epoch 8: model accuracy 97.63%\n",
            "Epoch 9 --> loss = 0.0626091555116475\n",
            "Epoch 9: model accuracy 97.97%\n",
            "Epoch 10 --> loss = 0.060556702526132725\n",
            "Epoch 10: model accuracy 98.08%\n",
            "Epoch 11 --> loss = 0.058724651238727284\n",
            "Epoch 11: model accuracy 98.37%\n",
            "Epoch 12 --> loss = 0.05680903870930439\n",
            "Epoch 12: model accuracy 98.10%\n",
            "Epoch 13 --> loss = 0.052531184530623186\n",
            "Epoch 13: model accuracy 97.89%\n",
            "Epoch 14 --> loss = 0.053380317670611355\n",
            "Epoch 14: model accuracy 98.14%\n",
            "Epoch 15 --> loss = 0.0521115291320968\n",
            "Epoch 15: model accuracy 98.19%\n",
            "Finished Training\n",
            "Epoch 1 --> loss = 0.3564766731629769\n",
            "Epoch 1: model accuracy 95.27%\n",
            "Epoch 2 --> loss = 0.12933114745145044\n",
            "Epoch 2: model accuracy 96.85%\n",
            "Epoch 3 --> loss = 0.10449128623095652\n",
            "Epoch 3: model accuracy 96.71%\n",
            "Epoch 4 --> loss = 0.09008697900967673\n",
            "Epoch 4: model accuracy 97.66%\n",
            "Epoch 5 --> loss = 0.0809961233247382\n",
            "Epoch 5: model accuracy 97.86%\n",
            "Epoch 6 --> loss = 0.0768023754900787\n",
            "Epoch 6: model accuracy 97.85%\n",
            "Epoch 7 --> loss = 0.07190313101013501\n",
            "Epoch 7: model accuracy 98.13%\n",
            "Epoch 8 --> loss = 0.0672304005674397\n",
            "Epoch 8: model accuracy 98.21%\n",
            "Epoch 9 --> loss = 0.06513209715636913\n",
            "Epoch 9: model accuracy 97.99%\n",
            "Epoch 10 --> loss = 0.06667387403029328\n",
            "Epoch 10: model accuracy 98.33%\n",
            "Epoch 11 --> loss = 0.05843715609324475\n",
            "Epoch 11: model accuracy 98.32%\n",
            "Epoch 12 --> loss = 0.058231179417530074\n",
            "Epoch 12: model accuracy 98.27%\n",
            "Epoch 13 --> loss = 0.05741228783748423\n",
            "Epoch 13: model accuracy 98.09%\n",
            "Epoch 14 --> loss = 0.053505277019809\n",
            "Epoch 14: model accuracy 98.38%\n",
            "Epoch 15 --> loss = 0.052931577995582485\n",
            "Epoch 15: model accuracy 98.08%\n",
            "Finished Training\n",
            "Epoch 1 --> loss = 0.30799420074044415\n",
            "Epoch 1: model accuracy 95.56%\n",
            "Epoch 2 --> loss = 0.1374117957885222\n",
            "Epoch 2: model accuracy 96.67%\n",
            "Epoch 3 --> loss = 0.11385801296799328\n",
            "Epoch 3: model accuracy 96.88%\n",
            "Epoch 4 --> loss = 0.10667891893566508\n",
            "Epoch 4: model accuracy 97.09%\n",
            "Epoch 5 --> loss = 0.0965999964135636\n",
            "Epoch 5: model accuracy 97.25%\n",
            "Epoch 6 --> loss = 0.09157619095589896\n",
            "Epoch 6: model accuracy 97.17%\n",
            "Epoch 7 --> loss = 0.08821377078717074\n",
            "Epoch 7: model accuracy 97.57%\n",
            "Epoch 8 --> loss = 0.08808608368232057\n",
            "Epoch 8: model accuracy 97.67%\n",
            "Epoch 9 --> loss = 0.08408653207419411\n",
            "Epoch 9: model accuracy 97.64%\n",
            "Epoch 10 --> loss = 0.08348358128368757\n",
            "Epoch 10: model accuracy 97.87%\n",
            "Epoch 11 --> loss = 0.0810371738570923\n",
            "Epoch 11: model accuracy 97.98%\n",
            "Epoch 12 --> loss = 0.07693377849446557\n",
            "Epoch 12: model accuracy 98.02%\n",
            "Epoch 13 --> loss = 0.07463869713386755\n",
            "Epoch 13: model accuracy 97.57%\n",
            "Epoch 14 --> loss = 0.08326869575021689\n",
            "Epoch 14: model accuracy 97.73%\n",
            "Epoch 15 --> loss = 0.0752193775716556\n",
            "Epoch 15: model accuracy 97.76%\n",
            "Finished Training\n"
          ]
        }
      ],
      "source": [
        "# I chose a lower number of epochs due to the limited resources and time for training\n",
        "n_epochs = 15\n",
        "results = []\n",
        "for learning_rate in [0.001, 0.005, 0.01]:\n",
        "    for batch_size in [32, 16]:\n",
        "        results.append(train_mnist(n_epochs=n_epochs, lr=learning_rate, batch_size=batch_size))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hyperparamters: learning_rate = 0.001, batch_size = 32\n",
            "Accuracy: 97.3499984741211\n",
            "Hyperparamters: learning_rate = 0.001, batch_size = 16\n",
            "Accuracy: 97.8699951171875\n",
            "Hyperparamters: learning_rate = 0.005, batch_size = 32\n",
            "Accuracy: 98.29999542236328\n",
            "Hyperparamters: learning_rate = 0.005, batch_size = 16\n",
            "Accuracy: 98.18999481201172\n",
            "Hyperparamters: learning_rate = 0.01, batch_size = 32\n",
            "Accuracy: 98.07999420166016\n",
            "Hyperparamters: learning_rate = 0.01, batch_size = 16\n",
            "Accuracy: 97.75999450683594\n"
          ]
        }
      ],
      "source": [
        "for result in results:\n",
        "    print(f'Hyperparamters: learning_rate = {result[\"lr\"]}, batch_size = {result[\"batch_size\"]}')\n",
        "    print(f'Accuracy: {result[\"accuracy\"]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As we can see the models are quite similar in terms of accuracy but the best-performing one has the hyperparameters: learning_rate = 0.005 and batch_size = 32\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
