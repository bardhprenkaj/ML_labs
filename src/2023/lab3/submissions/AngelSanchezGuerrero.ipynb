{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Challenge: Implement a Multiclass Classification Neural Network using PyTorch**\n",
        "\n",
        "Objective:\n",
        "Build a neural network using PyTorch to predict handwritten digits of MNIST.\n",
        "\n",
        "Steps:\n",
        "\n",
        "1. **Data Preparation**: Load the MNIST dataset using ```torchvision.datasets.MNIST```. Standardize/normalize the features. Split the dataset into training and testing sets using, for example, ```sklearn.model_selection.train_test_split()```. **Bonus scores**: *use PyTorch's built-* ```DataLoader``` *to split the dataset*.\n",
        "\n",
        "2. **Neural Network Architecture**: Define a simple feedforward neural network using PyTorch's ```nn.Module```. Design the input layer to match the number of features in the MNIST dataset and the output layer to have as many neurons as there are classes (10). You can experiment with the number of hidden layers and neurons to optimize the performance. **Bonus scores**: *Make your architecture flexibile to have as many hidden layers as the user wants, and use hyperparameter optimization to select the best number of hidden layeres.*\n",
        "\n",
        "3. **Loss Function and Optimizer**: Choose an appropriate loss function for multiclass classification. Select an optimizer, like SGD (Stochastic Gradient Descent) or Adam.\n",
        "\n",
        "4. **Training**: Write a training loop to iterate over the dataset.\n",
        "Forward pass the input through the network, calculate the loss, and perform backpropagation. Update the weights of the network using the chosen optimizer.\n",
        "\n",
        "5. **Testing**: Evaluate the trained model on the test set. Calculate the accuracy of the model.\n",
        "\n",
        "6. **Optimization**: Experiment with hyperparameters (learning rate, number of epochs, etc.) to optimize the model's performance. Consider adjusting the neural network architecture for better results. **Notice that you can't use the optimization algorithms from scikit-learn that we saw in lab1: e.g.,** ```GridSearchCV```.\n"
      ],
      "metadata": {
        "id": "E5AYt3Cn35zA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Data Preparation: Load the MNIST dataset using torchvision.datasets.MNIST. Standardize/normalize the features. Split the dataset into training and testing sets using, for example, sklearn.model_selection.train_test_split(). Bonus scores: use PyTorch's built- DataLoader to split the dataset."
      ],
      "metadata": {
        "id": "dQnia0VvQmhH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "5XLynrxJ33v6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Define a transform to normalize the data\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.5,), (0.5,))])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download and load the training data\n",
        "trainset = datasets.MNIST('/content/MNIST_data/', download=True, train=True, transform=transform)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DraRe3RCJBlU",
        "outputId": "0ca2d6e3-fce9-48f8-96ee-2bbb08591c4f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /content/MNIST_data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 71885523.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /content/MNIST_data/MNIST/raw/train-images-idx3-ubyte.gz to /content/MNIST_data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to /content/MNIST_data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 15867919.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /content/MNIST_data/MNIST/raw/train-labels-idx1-ubyte.gz to /content/MNIST_data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to /content/MNIST_data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 1648877/1648877 [00:00<00:00, 24091558.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /content/MNIST_data/MNIST/raw/t10k-images-idx3-ubyte.gz to /content/MNIST_data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to /content/MNIST_data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 4701512.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /content/MNIST_data/MNIST/raw/t10k-labels-idx1-ubyte.gz to /content/MNIST_data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import numpy as np\n",
        "\n",
        "# Define the size of the validation set\n",
        "validation_size = 0.2\n",
        "\n",
        "# Get the indices for training and validation\n",
        "num_train = len(trainset)\n",
        "indices = list(range(num_train))\n",
        "np.random.shuffle(indices)\n",
        "split = int(np.floor(validation_size * num_train))\n",
        "train_idx, valid_idx = indices[split:], indices[:split]\n",
        "\n",
        "# Define samplers for obtaining training and validation batches\n",
        "train_sampler = SubsetRandomSampler(train_idx)\n",
        "valid_sampler = SubsetRandomSampler(valid_idx)\n"
      ],
      "metadata": {
        "id": "70IEdxbsIeWw"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# How many samples per batch to load\n",
        "batch_size = 20\n",
        "\n",
        "# Prepare data loaders\n",
        "trainloader = DataLoader(trainset, batch_size=batch_size, sampler=train_sampler)\n",
        "validloader = DataLoader(trainset, batch_size=batch_size, sampler=valid_sampler)\n"
      ],
      "metadata": {
        "id": "BYbfBiSWIiHA"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Neural Network Architecture: Define a simple feedforward neural network using PyTorch's nn.Module. Design the input layer to match the number of features in the MNIST dataset and the output layer to have as many neurons as there are classes (10). You can experiment with the number of hidden layers and neurons to optimize the performance. Bonus scores: Make your architecture flexibile to have as many hidden layers as the user wants, and use hyperparameter optimization to select the best number of hidden layeres."
      ],
      "metadata": {
        "id": "k4IBFeIUQwbD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class MNIST_Net(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # Define layers\n",
        "        self.layer1 = nn.Linear(28*28, 128)  # First hidden layer with 128 neurons\n",
        "        self.act1 = nn.ReLU()                # Apply layer with ReLU activation for hidden layers\n",
        "        self.layer2 = nn.Linear(128, 64)     # Second hidden layer with 64 neurons\n",
        "        self.act2 = nn.ReLU()                # Apply layer with ReLU activation for hidden layers\n",
        "        self.output = nn.Linear(64, 10)      # Output layer with 10 neurons\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Flatten the input tensor\n",
        "        x = x.view(-1, 28*28)\n",
        "        x = self.act1(self.layer1(x))\n",
        "        x = self.act2(self.layer2(x))\n",
        "        x = self.output(x)\n",
        "        return x\n",
        "\n",
        "class MNIST_Net_Bonus(nn.Module):\n",
        "\n",
        "  def __init__(self, input_size, hidden_sizes, output_size):\n",
        "      super().__init__()\n",
        "\n",
        "      # Create a list of all layer sizes: input, hidden, and output\n",
        "      all_sizes = [input_size] + hidden_sizes + [output_size]\n",
        "\n",
        "      # Create layers dynamically\n",
        "      self.layers = nn.ModuleList()\n",
        "      for i in range(len(all_sizes) - 1):\n",
        "          self.layers.append(nn.Linear(all_sizes[i], all_sizes[i + 1]))\n",
        "\n",
        "  def forward(self, x):\n",
        "      # Flatten the input tensor\n",
        "      x = x.view(-1, 28*28)\n",
        "\n",
        "      # Pass data through all layers except for the last one using ReLU\n",
        "      for layer in self.layers[:-1]:\n",
        "          x = F.relu(layer(x))\n",
        "\n",
        "      # No activation function for the last layer (output layer)\n",
        "      x = self.layers[-1](x)\n",
        "\n",
        "      return x\n"
      ],
      "metadata": {
        "id": "DSiEZVFrQ48f"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model = MNIST_Net()\n",
        "model_bonus = MNIST_Net_Bonus(784, [128, 64], 10)"
      ],
      "metadata": {
        "id": "R_m0cbYiT0RJ"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Loss Function and Optimizer: Choose an appropriate loss function for multiclass classification. Select an optimizer, like SGD (Stochastic Gradient Descent) or Adam."
      ],
      "metadata": {
        "id": "OXrsVVyJVKvp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss function\n",
        "loss_function = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "N92sz3sWVNhQ"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# Optimizer - SGD\n",
        "# optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "# Optimizer - Adam\n",
        "optimizer = optim.Adam(model_bonus.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "jfDa4qXpXwto"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Training: Write a training loop to iterate over the dataset. Forward pass the input through the network, calculate the loss, and perform backpropagation. Update the weights of the network using the chosen optimizer."
      ],
      "metadata": {
        "id": "-dB2xRL6jmcq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of epochs\n",
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    # Loop over the dataset in batches\n",
        "    for inputs, labels in trainloader:  # trainloader is our DataLoader\n",
        "        # Forward pass\n",
        "        outputs = model_bonus(inputs)\n",
        "        loss = loss_function(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()  # Clear existing gradients\n",
        "        loss.backward()        # Compute gradients of all variables wrt loss\n",
        "        optimizer.step()       # Perform updates using calculated gradients\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-aDjKpqkKgp",
        "outputId": "94f2fc05-6eb6-49fa-f1a2-c7e63354dc5c"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5, Loss: 0.16604934632778168\n",
            "Epoch 2/5, Loss: 0.5756732225418091\n",
            "Epoch 3/5, Loss: 0.017475536093115807\n",
            "Epoch 4/5, Loss: 0.03351627662777901\n",
            "Epoch 5/5, Loss: 0.33252060413360596\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Testing: Evaluate the trained model on the test set. Calculate the accuracy of the model."
      ],
      "metadata": {
        "id": "GN18s3uuqGI5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Track the number of correct predictions\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "# Disable gradient computation; we don't need it for evaluation\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in validloader:  #  testloader is DataLoader for test set\n",
        "        # Forward pass\n",
        "        outputs = model_bonus(inputs)\n",
        "\n",
        "        # Get indexes of predictions from the maximum value\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "        # Total number of labels\n",
        "        total += labels.size(0)\n",
        "\n",
        "        # Total correct predictions\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "# Calculate the accuracy\n",
        "accuracy = 100 * correct / total\n",
        "print(f'Accuracy of the model on the test set: {accuracy}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iKAX1Z1yqIi9",
        "outputId": "237012e7-7804-4d5a-db06-cb7e1d95e1e8"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the model on the test set: 96.09166666666667%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Optimization: Experiment with hyperparameters (learning rate, number of epochs, etc.) to optimize the model's performance. Consider adjusting the neural network architecture for better results. Notice that you can't use the optimization algorithms from scikit-learn that we saw in lab1: e.g., GridSearchCV."
      ],
      "metadata": {
        "id": "FqNrGUBAe3lD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KvQPiZv-_d4o",
        "outputId": "7fa386f9-5ade-484a-c028-42485ad2005e"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-3.5.0-py3-none-any.whl (413 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m413.4/413.4 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.13.1-py3-none-any.whl (233 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.4/233.4 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.8.0-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (23.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.23)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.66.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.1)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna)\n",
            "  Downloading Mako-1.3.0-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.5.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna) (3.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.1.3)\n",
            "Installing collected packages: Mako, colorlog, alembic, optuna\n",
            "Successfully installed Mako-1.3.0 alembic-1.13.1 colorlog-6.8.0 optuna-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "\n",
        "def objective(trial):\n",
        "\n",
        "    # Hyperparameters to optimize\n",
        "    learning_rate = trial.suggest_categorical('learning_rate', [0.0001, 0.001, 0.01])\n",
        "    num_epochs = trial.suggest_categorical('num_epochs', [5, 10, 15, 20])\n",
        "    hidden_size = trial.suggest_categorical('hidden_size', [[128, 64], [128], [128, 64, 16]])\n",
        "    batch_size = trial.suggest_categorical('batch_size', [64, 256, 512, 1024, 2048])\n",
        "\n",
        "    trainloader = DataLoader(trainset, batch_size=batch_size, sampler=train_sampler)\n",
        "    validloader = DataLoader(trainset, batch_size=batch_size, sampler=valid_sampler)\n",
        "    input_size = 28*28\n",
        "    output_size = 10\n",
        "    model_optimize = MNIST_Net_Bonus(input_size, hidden_size, output_size)\n",
        "    optimizer = optim.Adam(model_optimize.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Training loop\n",
        "    model_bonus.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        for inputs, labels in trainloader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model_optimize(inputs)\n",
        "            loss = loss_function(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    # evaluation\n",
        "    model_optimize.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in validloader:\n",
        "            outputs = model_optimize(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "\n",
        "    return accuracy\n",
        "\n",
        "# Create a study object and optimize\n",
        "study = optuna.create_study(direction='maximize')\n",
        "study.optimize(objective, n_trials=10)\n",
        "\n",
        "# Get the best hyperparameters\n",
        "best_params = study.best_params\n",
        "best_learning_rate = best_params['learning_rate']\n",
        "best_num_epochs = best_params['num_epochs']\n",
        "best_hidden_size = best_params['hidden_size']\n",
        "best_batch_size = best_params['batch_size']\n",
        "\n",
        "\n",
        "print(\"Best Hyperparameters:\")\n",
        "print(f\"Learning Rate: {best_learning_rate}\")\n",
        "print(f\"Number of Epochs: {best_num_epochs}\")\n",
        "print(f\"Hidden Size: {best_hidden_size}\")\n",
        "print(f\"Batch Size: {best_batch_size}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zxVYnkIO_4kW",
        "outputId": "6ae04f23-05b6-4932-a287-84ae31700e28"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-12-21 16:03:32,994] A new study created in memory with name: no-name-facf2b60-7068-439e-b818-a663fa7b4c3c\n",
            "/usr/local/lib/python3.10/dist-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [128, 64] which is of type list.\n",
            "  warnings.warn(message)\n",
            "/usr/local/lib/python3.10/dist-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [128] which is of type list.\n",
            "  warnings.warn(message)\n",
            "/usr/local/lib/python3.10/dist-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [128, 64, 16] which is of type list.\n",
            "  warnings.warn(message)\n",
            "[I 2023-12-21 16:08:35,669] Trial 0 finished with value: 94.49166666666666 and parameters: {'learning_rate': 0.01, 'num_epochs': 15, 'hidden_size': [128, 64, 16], 'batch_size': 64}. Best is trial 0 with value: 94.49166666666666.\n",
            "[I 2023-12-21 16:14:12,282] Trial 1 finished with value: 93.44166666666666 and parameters: {'learning_rate': 0.0001, 'num_epochs': 20, 'hidden_size': [128, 64], 'batch_size': 512}. Best is trial 0 with value: 94.49166666666666.\n",
            "[I 2023-12-21 16:18:33,531] Trial 2 finished with value: 97.175 and parameters: {'learning_rate': 0.001, 'num_epochs': 15, 'hidden_size': [128, 64], 'batch_size': 256}. Best is trial 2 with value: 97.175.\n",
            "[I 2023-12-21 16:20:12,917] Trial 3 finished with value: 92.80833333333334 and parameters: {'learning_rate': 0.0001, 'num_epochs': 5, 'hidden_size': [128], 'batch_size': 64}. Best is trial 2 with value: 97.175.\n",
            "[I 2023-12-21 16:24:25,281] Trial 4 finished with value: 95.48333333333333 and parameters: {'learning_rate': 0.01, 'num_epochs': 15, 'hidden_size': [128, 64, 16], 'batch_size': 2048}. Best is trial 2 with value: 97.175.\n",
            "[I 2023-12-21 16:28:44,703] Trial 5 finished with value: 96.65833333333333 and parameters: {'learning_rate': 0.001, 'num_epochs': 15, 'hidden_size': [128, 64], 'batch_size': 256}. Best is trial 2 with value: 97.175.\n",
            "[I 2023-12-21 16:34:14,881] Trial 6 finished with value: 91.29166666666667 and parameters: {'learning_rate': 0.0001, 'num_epochs': 20, 'hidden_size': [128], 'batch_size': 1024}. Best is trial 2 with value: 97.175.\n",
            "[I 2023-12-21 16:37:06,831] Trial 7 finished with value: 89.36666666666666 and parameters: {'learning_rate': 0.0001, 'num_epochs': 10, 'hidden_size': [128], 'batch_size': 1024}. Best is trial 2 with value: 97.175.\n",
            "[I 2023-12-21 16:41:18,612] Trial 8 finished with value: 91.44166666666666 and parameters: {'learning_rate': 0.0001, 'num_epochs': 15, 'hidden_size': [128, 64, 16], 'batch_size': 512}. Best is trial 2 with value: 97.175.\n",
            "[I 2023-12-21 16:47:51,420] Trial 9 finished with value: 97.35 and parameters: {'learning_rate': 0.001, 'num_epochs': 20, 'hidden_size': [128, 64, 16], 'batch_size': 64}. Best is trial 9 with value: 97.35.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters:\n",
            "Learning Rate: 0.001\n",
            "Number of Epochs: 20\n",
            "Hidden Size: [128, 64, 16]\n",
            "Batch Size: 64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This required a lot of time (44 min) but I was in no rush."
      ],
      "metadata": {
        "id": "-aKd3mVBJlhH"
      }
    }
  ]
}