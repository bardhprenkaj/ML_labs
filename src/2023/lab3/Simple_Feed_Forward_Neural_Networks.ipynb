{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Load Data\n",
        "\n",
        "The first step is to define the functions and classes you intend to use in this post. You will use the NumPy library to load your dataset and the  PyTorch library for deep learning models.\n",
        "\n",
        "The imports required are listed below:"
      ],
      "metadata": {
        "id": "SwyGzdVvwuCb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "m89_Vw1gwIr7"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we will use the Pima Indians onset of diabetes dataset. This has been a standard machine learning dataset since the early days of the field. It describes patient medical record data for Pima Indians and whether they had an onset of diabetes within five years.\n",
        "\n",
        "It is a **binary classification problem** (onset of diabetes as 1 or not as 0). All the input variables that describe each patient are transformed and numerical. This makes it easy to use directly with neural networks that expect numerical input and output values and is an ideal choice for our first neural network in **PyTorch**.\n",
        "\n",
        "The data can be downloaded [here](https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv). You can also find it under ```resources/2023_24/lab3/pima-indians-diabetes.csv```"
      ],
      "metadata": {
        "id": "PVXvE9Tfwr4i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download the dataset and place it in your local working directory, the same location as your Python file. Save it with the filename ```pima-indians-diabetes.csv```. Take a look inside the file; you should see rows of data like the following:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "6,148,72,35,0,33.6,0.627,50,1\n",
        "1,85,66,29,0,26.6,0.351,31,0\n",
        "8,183,64,0,0,23.3,0.672,32,1\n",
        "1,89,66,23,94,28.1,0.167,21,0\n",
        "0,137,40,35,168,43.1,2.288,33,1\n",
        "...\n",
        "```"
      ],
      "metadata": {
        "id": "VlgerK7wxPsm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can now load the file as a matrix of numbers using the NumPy function ```loadtxt()```. There are eight input variables and one output variable (the last column). You will be learning a model to map rows of input variables ($X$) to an output variable ($y$), which is often summarized as $y = f(X)$. The variables are summarized as follows:\n",
        "\n",
        "Input Variables ($X$):\n",
        "\n",
        "1. Number of times pregnant\n",
        "2. Plasma glucose concentration at 2 hours in an oral glucose tolerance test\n",
        "3. Diastolic blood pressure (mm Hg)\n",
        "4. Triceps skin fold thickness (mm)\n",
        "5. 2-hour serum insulin (μIU/ml)\n",
        "6. Body mass index $\\big(\\frac{\\text{weight in kg}}{\\text{height in kg}^2}\\big)$\n",
        "7. Diabetes pedigree function\n",
        "8. Age (years)\n",
        "\n",
        "Output Variables ($y$):\n",
        "* Class label (0 or 1)"
      ],
      "metadata": {
        "id": "NE5eXSCjxX9F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once the CSV file is loaded into memory, you can split the columns of data into input and output variables.\n",
        "\n",
        "The data will be stored in a 2D array where the first dimension is rows and the second dimension is columns, e.g., (rows, columns). You can split the array into two arrays by selecting subsets of columns using the standard NumPy slice operator “:“. You can select the first eight columns from index 0 to index 7 via the slice 0:8. You can then select the output column (the 9th variable) via index 8."
      ],
      "metadata": {
        "id": "sBOhNXJcx-6r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # load the dataset, split into input (X) and output (y) variables\n",
        "dataset = np.loadtxt('pima-indians-diabetes.csv', delimiter=',')\n",
        "X = dataset[:,0:8]\n",
        "y = dataset[:,8]"
      ],
      "metadata": {
        "id": "O7yjVDYmyDlw"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "But these data should be converted to PyTorch tensors first. One reason is that PyTorch usually operates in a 32-bit floating point while NumPy, by default, uses a 64-bit floating point. Mix-and-match is not allowed in most operations. Converting to PyTorch tensors can avoid the implicit conversion that may cause problems.\n",
        "\n",
        "You can also take this chance to correct the shape to fit what PyTorch would expect, e.g., prefer $n \\times 1$ matrix over $n$-vectors.\n",
        "\n"
      ],
      "metadata": {
        "id": "kZATrZLUyWGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To convert, create a tensor out of NumPy arrays:"
      ],
      "metadata": {
        "id": "eqIQSCnkye4K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.tensor(X, dtype=torch.float32)\n",
        "y = torch.tensor(y, dtype=torch.float32).reshape(-1, 1)"
      ],
      "metadata": {
        "id": "8BH_uc8vyQfs"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define the Neural Network Model\n",
        "\n",
        "\n",
        "Indeed, there are two ways to define a model in PyTorch. The goal is to make it like a function that takes an input and returns an output.\n",
        "\n",
        "A model can be defined as a sequence of layers. You create a ```Sequential``` model with the layers listed out. The first thing you need to do to get this right is to ensure the first layer has the correct number of input features. In this example, you can specify the input dimension  8 for the eight input variables as one vector.\n",
        "\n",
        "The other parameters for a layer or how many layers you need for a model is not an easy question. You may use heuristics to help you design the model, or you can refer to other people’s designs in dealing with a similar problem. Often, the best neural network structure is found through a process of trial-and-error experimentation. Generally, you need a network large enough to capture the structure of the problem but small enough to make it fast. In this example, let’s use a fully-connected network structure with three layers.\n",
        "\n",
        "Fully connected layers or dense layers are defined using the ```Linear``` class in PyTorch. It simply means an operation similar to matrix multiplication. You can specify the number of inputs as the first argument and the number of outputs as the second argument. The number of outputs is sometimes called the number of neurons or number of nodes in the layer.\n",
        "\n",
        "You also need an activation function after the layer. If not provided, you just take the output of the matrix multiplication to the next step, or sometimes you call it using linear activation, hence the name of the layer.\n",
        "\n",
        "In this example, you will use the rectified linear unit activation function, referred to as ```ReLU```, on the first two layers and the sigmoid function in the output layer.\n",
        "\n",
        "**A sigmoid on the output layer ensures the output is between 0 and 1**, which is easy to map to either a probability of class 1 or snap to a hard classification of either class by a cut-off threshold of 0.5.\n",
        "\n",
        "*In the past, you might have used sigmoid and tanh activation functions for all layers, but it turns out that sigmoid activation can lead to the problem of vanishing gradient in deep neural networks, and ReLU activation is found to provide better performance in terms of both speed and accuracy.*"
      ],
      "metadata": {
        "id": "YihVXw35ylac"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Piecing it all together by adding each layer we have:\n",
        "* The model expects rows of data with 8 variables (the first argument at the first layer set to 8)\n",
        "* The first hidden layer has 12 neurons, followed by a ReLU activation function\n",
        "* The second hidden layer has 8 neurons, followed by another ReLU activation function\n",
        "*The output layer has one neuron, followed by a sigmoid activation function"
      ],
      "metadata": {
        "id": "XG8D6aCyzDQq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = nn.Sequential(\n",
        "    nn.Linear(8, 12),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(12, 8),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(8, 1),\n",
        "    nn.Sigmoid()\n",
        ")"
      ],
      "metadata": {
        "id": "76jTFvUHygyW"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kZjegEeEzM0q",
        "outputId": "30dc8bae-68b0-4cd7-a8bd-32d12cb9b37c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Linear(in_features=8, out_features=12, bias=True)\n",
              "  (1): ReLU()\n",
              "  (2): Linear(in_features=12, out_features=8, bias=True)\n",
              "  (3): ReLU()\n",
              "  (4): Linear(in_features=8, out_features=1, bias=True)\n",
              "  (5): Sigmoid()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You are free to change the design and see if you get a better or worse result than the subsequent part of this post.\n",
        "\n",
        "But note that, in PyTorch, there is a more verbose way of creating a model. The model above can be created as a Python class inherited from the ```nn.Module```:"
      ],
      "metadata": {
        "id": "Wu7m1MIMzSp6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyFirstNeuralNetwork(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    self.first_layer = nn.Linear(8,12)\n",
        "    self.act1 = nn.ReLU()\n",
        "    self.second_layer = nn.Linear(12,8)\n",
        "    self.act2 = nn.ReLU()\n",
        "    self.output = nn.Linear(8,1)\n",
        "    self.output_act = nn.Sigmoid()\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.act1(self.first_layer(x))\n",
        "    x = self.act2(self.second_layer(x))\n",
        "    x = self.output_act(self.output(x))\n",
        "    return x"
      ],
      "metadata": {
        "id": "loInbeNxzPWJ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MyFirstNeuralNetwork()"
      ],
      "metadata": {
        "id": "PKZaiZ5Oz3_9"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hCRSdVysz6Kb",
        "outputId": "5d8a9ab5-21e0-4824-a425-db921fd5d883"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MyFirstNeuralNetwork(\n",
              "  (first_layer): Linear(in_features=8, out_features=12, bias=True)\n",
              "  (act1): ReLU()\n",
              "  (second_layer): Linear(in_features=12, out_features=8, bias=True)\n",
              "  (act2): ReLU()\n",
              "  (output): Linear(in_features=8, out_features=1, bias=True)\n",
              "  (output_act): Sigmoid()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this approach, a class needs to have all the layers defined in the constructor because you need to prepare all its components when it is created, but the input is not yet provided. Note that you also need to call the parent class's constructor (the line ```super().__init__()```) to bootstrap your model. You also need to define a ```forward()``` function in the class to tell, if an input tensor ```x``` is provided, how you produce the output tensor in return."
      ],
      "metadata": {
        "id": "WrIp1VFSz9Xi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can see from the output above that the model remembers how you call each layer."
      ],
      "metadata": {
        "id": "lf68NDbv0GJP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparation for Training\n",
        "\n",
        "A defined model is ready for training, but you need to specify what the goal of the training is. In this example, the data has the input features $X$ and the output label $y$. You want the neural network model to produce an output that is as close to $y$ as possible. Training a network means finding the best set of weights to map inputs to outputs in your dataset. The loss function is the metric to measure the prediction's distance to $y$. In this example, you should use binary cross entropy because it is a binary classification problem.\n",
        "\n",
        "Once you decide on the loss function, you also need an optimizer. The optimizer is the algorithm you use to adjust the model weights progressively to produce a better output. There are many optimizers to choose from, and in this example, **Adam** is used. This popular version of gradient descent can automatically tune itself and gives good results in a wide range of problems."
      ],
      "metadata": {
        "id": "SZmmf19a0H1d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.BCELoss()  # binary cross entropy\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "s3j7YPLCz6tL"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The optimizer usually has some configuration parameters. Most notably, the learning rate lr. But all optimizers need to know what to optimize. Therefore. you pass on ```model.parameters()```, which is a generator of all parameters from the model you created."
      ],
      "metadata": {
        "id": "uT8ds19w0ZPl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training our Model\n",
        "\n",
        "You have defined your model, the loss metric, and the optimizer. It is ready for training by executing the model on some data.\n",
        "\n",
        "Training a neural network model usually takes in epochs and batches. They are idioms for how data is passed to a model:\n",
        "\n",
        "* **Epoch**: Passes the entire training dataset to the model once\n",
        "* **Batch**: One or more samples passed to the model, from which the gradient descent algorithm will be executed for one iteration\n",
        "\n",
        "Simply speaking, the entire dataset is split into batches, and you pass the batches one by one into a model using a training loop. Once you have exhausted all the batches, you have finished one epoch. Then you can start over again with the same dataset and start the second epoch, continuing to refine the model. This process repeats until you are satisfied with the model's output.\n",
        "\n",
        "The size of a batch is limited by the system's memory. Also, the number of computations required is linearly proportional to the size of a batch. The total number of batches over many epochs is how many times you run the gradient descent to refine the model. It is a trade-off that you want more iterations for the gradient descent so you can produce a better model, but at the same time, you do not want the training to take too long to complete. The number of epochs and the size of a batch can be chosen experimentally by trial and error.\n",
        "\n",
        "The goal of training a model is to ensure it learns a good enough mapping of input data to output classification. It will not be perfect, and errors are inevitable. Usually, you will see the amount of error reducing when in the later epochs, but it will eventually level out. This is called model convergence.\n",
        "\n",
        "The simplest way to build a training loop is to use two nested for-loops, one for epochs and one for batches:"
      ],
      "metadata": {
        "id": "EzJS9pfu0c59"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs = 100\n",
        "batch_size = 10\n",
        "\n",
        "# Training loop over a specified number of epochs\n",
        "for epoch in range(n_epochs):\n",
        "    # Loop over the dataset in batches\n",
        "    for i in range(0, len(X), batch_size):\n",
        "        # Extract a batch of input data\n",
        "        Xbatch = X[i:i+batch_size]\n",
        "\n",
        "        # Forward pass: Predict the output using the current model\n",
        "        y_pred = model(Xbatch)\n",
        "\n",
        "        # Extract the corresponding batch of target values\n",
        "        ybatch = y[i:i+batch_size]\n",
        "\n",
        "        # Compute the loss between the predicted and actual values\n",
        "        loss = loss_fn(y_pred, ybatch)\n",
        "\n",
        "        # Zero the gradients accumulated in previous iterations\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Backward pass: Compute the gradient of the loss with respect to model parameters\n",
        "        loss.backward()\n",
        "\n",
        "        # Update the model parameters using the computed gradients\n",
        "        optimizer.step()\n",
        "\n",
        "    # Print information about the current epoch\n",
        "    print(f'Finished epoch {epoch}, latest loss {loss}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C481U5f90W8f",
        "outputId": "474b16cc-f055-49f9-cea3-e10b02b8d09a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished epoch 0, latest loss 0.5453848242759705\n",
            "Finished epoch 1, latest loss 0.5137174129486084\n",
            "Finished epoch 2, latest loss 0.4802509844303131\n",
            "Finished epoch 3, latest loss 0.4633190333843231\n",
            "Finished epoch 4, latest loss 0.4544704854488373\n",
            "Finished epoch 5, latest loss 0.447897732257843\n",
            "Finished epoch 6, latest loss 0.43749678134918213\n",
            "Finished epoch 7, latest loss 0.4320908784866333\n",
            "Finished epoch 8, latest loss 0.426523894071579\n",
            "Finished epoch 9, latest loss 0.4219015836715698\n",
            "Finished epoch 10, latest loss 0.41824185848236084\n",
            "Finished epoch 11, latest loss 0.4185582995414734\n",
            "Finished epoch 12, latest loss 0.4186594486236572\n",
            "Finished epoch 13, latest loss 0.41769519448280334\n",
            "Finished epoch 14, latest loss 0.4198245406150818\n",
            "Finished epoch 15, latest loss 0.4204174876213074\n",
            "Finished epoch 16, latest loss 0.42096951603889465\n",
            "Finished epoch 17, latest loss 0.4197819232940674\n",
            "Finished epoch 18, latest loss 0.4169014096260071\n",
            "Finished epoch 19, latest loss 0.41948091983795166\n",
            "Finished epoch 20, latest loss 0.4173486828804016\n",
            "Finished epoch 21, latest loss 0.4169742465019226\n",
            "Finished epoch 22, latest loss 0.41539961099624634\n",
            "Finished epoch 23, latest loss 0.41563880443573\n",
            "Finished epoch 24, latest loss 0.41546347737312317\n",
            "Finished epoch 25, latest loss 0.415025919675827\n",
            "Finished epoch 26, latest loss 0.41684162616729736\n",
            "Finished epoch 27, latest loss 0.4195157587528229\n",
            "Finished epoch 28, latest loss 0.41906607151031494\n",
            "Finished epoch 29, latest loss 0.42014551162719727\n",
            "Finished epoch 30, latest loss 0.41994351148605347\n",
            "Finished epoch 31, latest loss 0.42996957898139954\n",
            "Finished epoch 32, latest loss 0.4221382737159729\n",
            "Finished epoch 33, latest loss 0.43816712498664856\n",
            "Finished epoch 34, latest loss 0.4242898225784302\n",
            "Finished epoch 35, latest loss 0.4253854751586914\n",
            "Finished epoch 36, latest loss 0.42403465509414673\n",
            "Finished epoch 37, latest loss 0.4387339949607849\n",
            "Finished epoch 38, latest loss 0.42937323451042175\n",
            "Finished epoch 39, latest loss 0.4365561306476593\n",
            "Finished epoch 40, latest loss 0.42788878083229065\n",
            "Finished epoch 41, latest loss 0.42476099729537964\n",
            "Finished epoch 42, latest loss 0.42645034193992615\n",
            "Finished epoch 43, latest loss 0.42858412861824036\n",
            "Finished epoch 44, latest loss 0.42715466022491455\n",
            "Finished epoch 45, latest loss 0.42941567301750183\n",
            "Finished epoch 46, latest loss 0.4599057734012604\n",
            "Finished epoch 47, latest loss 0.4418736398220062\n",
            "Finished epoch 48, latest loss 0.4372067153453827\n",
            "Finished epoch 49, latest loss 0.43912050127983093\n",
            "Finished epoch 50, latest loss 0.43808239698410034\n",
            "Finished epoch 51, latest loss 0.4388526380062103\n",
            "Finished epoch 52, latest loss 0.46002814173698425\n",
            "Finished epoch 53, latest loss 0.44189393520355225\n",
            "Finished epoch 54, latest loss 0.4465144872665405\n",
            "Finished epoch 55, latest loss 0.4445669651031494\n",
            "Finished epoch 56, latest loss 0.44423559308052063\n",
            "Finished epoch 57, latest loss 0.4535829424858093\n",
            "Finished epoch 58, latest loss 0.4500952363014221\n",
            "Finished epoch 59, latest loss 0.4502672553062439\n",
            "Finished epoch 60, latest loss 0.44689881801605225\n",
            "Finished epoch 61, latest loss 0.44777989387512207\n",
            "Finished epoch 62, latest loss 0.44808340072631836\n",
            "Finished epoch 63, latest loss 0.44936665892601013\n",
            "Finished epoch 64, latest loss 0.45036518573760986\n",
            "Finished epoch 65, latest loss 0.4500578045845032\n",
            "Finished epoch 66, latest loss 0.4526200294494629\n",
            "Finished epoch 67, latest loss 0.4689858555793762\n",
            "Finished epoch 68, latest loss 0.4539821743965149\n",
            "Finished epoch 69, latest loss 0.452170193195343\n",
            "Finished epoch 70, latest loss 0.45339465141296387\n",
            "Finished epoch 71, latest loss 0.4519583582878113\n",
            "Finished epoch 72, latest loss 0.4574735462665558\n",
            "Finished epoch 73, latest loss 0.4559755027294159\n",
            "Finished epoch 74, latest loss 0.4549005925655365\n",
            "Finished epoch 75, latest loss 0.455452561378479\n",
            "Finished epoch 76, latest loss 0.4639813303947449\n",
            "Finished epoch 77, latest loss 0.4538250267505646\n",
            "Finished epoch 78, latest loss 0.4539172649383545\n",
            "Finished epoch 79, latest loss 0.4610297381877899\n",
            "Finished epoch 80, latest loss 0.4581550657749176\n",
            "Finished epoch 81, latest loss 0.4578362703323364\n",
            "Finished epoch 82, latest loss 0.45731019973754883\n",
            "Finished epoch 83, latest loss 0.4556785225868225\n",
            "Finished epoch 84, latest loss 0.4590132236480713\n",
            "Finished epoch 85, latest loss 0.4575687646865845\n",
            "Finished epoch 86, latest loss 0.45772355794906616\n",
            "Finished epoch 87, latest loss 0.4623569846153259\n",
            "Finished epoch 88, latest loss 0.46190914511680603\n",
            "Finished epoch 89, latest loss 0.459626704454422\n",
            "Finished epoch 90, latest loss 0.4608825445175171\n",
            "Finished epoch 91, latest loss 0.4617994427680969\n",
            "Finished epoch 92, latest loss 0.46104300022125244\n",
            "Finished epoch 93, latest loss 0.46238642930984497\n",
            "Finished epoch 94, latest loss 0.46016427874565125\n",
            "Finished epoch 95, latest loss 0.46202924847602844\n",
            "Finished epoch 96, latest loss 0.45892247557640076\n",
            "Finished epoch 97, latest loss 0.4546930193901062\n",
            "Finished epoch 98, latest loss 0.4596375823020935\n",
            "Finished epoch 99, latest loss 0.4596096873283386\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Outer Loop (Epochs)**:\n",
        "- ```for epoch in range(n_epochs)```: initiates a loop over a specified number of training epochs. An epoch is a single pass through the entire dataset.\n",
        "\n",
        "2. **Inner Loop (Batches)**:\n",
        "- ```for i in range(0, len(X), batch_size)```: iterates over the dataset in batches of size ```batch_size```. This is a common practice to efficiently train neural networks on large datasets.\n",
        "\n",
        "3. **Extract Batch Data**:\n",
        "- ```Xbatch = X[i:i+batch_size]``` extracts a batch of input data (```X```) based on the current iteration index ```i```. It slices the data to get a batch of size ```batch_size```.\n",
        "\n",
        "4. **Forward Pass**:\n",
        "- ```y_pred = model(Xbatch)``` performs a forward pass through the neural network (```model```) to obtain predictions (```y_pred```) for the current batch of input data.\n",
        "\n",
        "5. **Extract Batch Targets**:\n",
        "- ```ybatch = y[i:i+batch_size]``` extracts the corresponding batch of target values from the ground truth (```y```) based on the current iteration index ```i```.\n",
        "\n",
        "6. **Compute Loss**:\n",
        "- ```loss = loss_fn(y_pred, ybatch)``` calculates the loss between the predicted values (```y_pred```) and the actual targets (```ybatch```) using a specified loss function (```loss_fn```).\n",
        "\n",
        "7. **Zero Gradients**:\n",
        "- ```optimizer.zero_grad()``` clears the gradients of all optimized tensors. This is necessary before computing the gradients for the current batch.\n",
        "\n",
        "8. **Backward Pass (Backpropagation)**:\n",
        "- ```loss.backward()``` computes the gradient of the loss with respect to the model parameters through backpropagation.\n",
        "\n",
        "9. **Update Parameters**:\n",
        "- ```optimizer.step()``` updates the model parameters using the computed gradients and the optimization algorithm (e.g., stochastic gradient descent)\n",
        "\n"
      ],
      "metadata": {
        "id": "0EjEb-i86J6g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate the Model\n",
        "\n",
        "You have trained our neural network on the entire dataset, and you can evaluate the performance of the network on the same dataset. This will only give you an idea of how well you have modeled the dataset (e.g., train accuracy) but no idea of how well the algorithm might perform on new data. This was done for simplicity, but ideally, you could separate your data into train and test datasets for training and evaluation of your model.\n",
        "\n",
        "You can evaluate your model on your training dataset in the same way you invoked the model in training. This will generate predictions for each input, but then you still need to compute a score for the evaluation. This score can be the same as your loss function or something different. Because you are doing binary classification, you can use accuracy as your evaluation score by converting the output (a floating point in the range of 0 to 1) to an integer (0 or 1) and compare to the label we know."
      ],
      "metadata": {
        "id": "PEgOYuz006_t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# compute accuracy (no_grad is optional)\n",
        "with torch.no_grad():\n",
        "  y_pred = model(X)\n",
        "\n",
        "accuracy = (y_pred.round() == y).float().mean()\n",
        "print(f\"Accuracy {accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pTNHXL2e02Zs",
        "outputId": "c595822e-b1e5-404f-b204-b2c81c5e1d0b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy 0.76171875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The ```round()``` function rounds off the floating point to the nearest integer. The ```==``` operator compares and returns a Boolean tensor, which can be converted to floating point numbers 1.0 and 0.0. The ```mean()``` function will provide you the count of the number of 1's (i.e., prediction matches the label) divided by the total number of samples. The ```no_grad()``` context is optional but suggested, so you relieve ```y_pred``` from remembering how it comes up with the number since you are not going to do differentiation on it."
      ],
      "metadata": {
        "id": "RMjuUHaP1Gms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Make Predictions\n",
        "\n",
        "We can generate predictions on the training dataset, pretending it is a new dataset you have not seen before. Making predictions is as easy as calling the model as if it is a function. You are using a sigmoid activation function on the output layer so that the predictions will be a probability in the range between 0 and 1."
      ],
      "metadata": {
        "id": "VekTyiqs1ZQp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# make class predictions with the model\n",
        "predictions = (model(X) > 0.5).int()\n",
        "for i in range(5):\n",
        "    print('%s => %d (expected %d)' % (X[i].tolist(), predictions[i], y[i]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tuYvdVio1iPc",
        "outputId": "736b6e14-f6c2-402d-b3c1-fcf48cb54d34"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[6.0, 148.0, 72.0, 35.0, 0.0, 33.599998474121094, 0.6269999742507935, 50.0] => 1 (expected 1)\n",
            "[1.0, 85.0, 66.0, 29.0, 0.0, 26.600000381469727, 0.35100001096725464, 31.0] => 0 (expected 0)\n",
            "[8.0, 183.0, 64.0, 0.0, 0.0, 23.299999237060547, 0.671999990940094, 32.0] => 1 (expected 1)\n",
            "[1.0, 89.0, 66.0, 23.0, 94.0, 28.100000381469727, 0.16699999570846558, 21.0] => 0 (expected 0)\n",
            "[0.0, 137.0, 40.0, 35.0, 168.0, 43.099998474121094, 2.2880001068115234, 33.0] => 1 (expected 1)\n"
          ]
        }
      ]
    }
  ]
}