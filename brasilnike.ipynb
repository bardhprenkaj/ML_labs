{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5AYt3Cn35zA"
      },
      "source": [
        "**Challenge: Implement a Multiclass Classification Neural Network using PyTorch**\n",
        "\n",
        "Objective:\n",
        "Build a feedforward neural network using PyTorch to predict the species of iris flowers in a multiclass classification problem. The dataset used for this challenge is the Iris dataset, which consists of features like sepal length, sepal width, petal length, and petal width.\n",
        "\n",
        "Steps:\n",
        "\n",
        "1. **Data Preparation**: Load the MNIST dataset using ```torchvision.datasets.MNIST```. Standardize/normalize the features. Split the dataset into training and testing sets using, for example, ```sklearn.model_selection.train_test_split()```. **Bonus scores**: *use PyTorch's built-* ```DataLoader``` *to split the dataset*.\n",
        "\n",
        "2. **Neural Network Architecture**: Define a simple feedforward neural network using PyTorch's ```nn.Module```. Design the input layer to match the number of features in the MNIST dataset and the output layer to have as many neurons as there are classes (10). You can experiment with the number of hidden layers and neurons to optimize the performance. **Bonus scores**: *Make your architecture flexibile to have as many hidden layers as the user wants, and use hyperparameter optimization to select the best number of hidden layeres.*\n",
        "\n",
        "3. **Loss Function and Optimizer**: Choose an appropriate loss function for multiclass classification. Select an optimizer, like SGD (Stochastic Gradient Descent) or Adam.\n",
        "\n",
        "4. **Training**: Write a training loop to iterate over the dataset.\n",
        "Forward pass the input through the network, calculate the loss, and perform backpropagation. Update the weights of the network using the chosen optimizer.\n",
        "\n",
        "5. **Testing**: Evaluate the trained model on the test set. Calculate the accuracy of the model.\n",
        "\n",
        "6. **Optimization**: Experiment with hyperparameters (learning rate, number of epochs, etc.) to optimize the model's performance. Consider adjusting the neural network architecture for better results. **Notice that you can't use the optimization algorithms from scikit-learn that we saw in lab1: e.g.,** ```GridSearchCV```.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "5XLynrxJ33v6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision.datasets import MNIST\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import torch.nn as nn\n",
        "\n",
        "# Load the MNIST dataset\n",
        "mnist = MNIST(root='data/', train=True, download=True)\n",
        "\n",
        "# Get the features and labels\n",
        "features = mnist.data\n",
        "labels = mnist.targets\n",
        "\n",
        "# Normalize the features\n",
        "features = features.float() / 255.0\n",
        "\n",
        "# Flatten the features and convert them to tensors\n",
        "features = features.view(-1, 28*28).clone().detach()\n",
        "\n",
        "# Convert the labels to tensors\n",
        "labels = labels.clone().detach()\n",
        "\n",
        "# Combine features and labels into a dataset\n",
        "dataset = torch.utils.data.TensorDataset(features, labels)\n",
        "# Split the dataset into training and testing sets using DataLoader\n",
        "train_size = int(0.8 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
        "\n",
        "# Create DataLoader for training and testing sets\n",
        "batch_size = 64 #64\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. **Neural Network Architecture**: Define a simple feedforward neural network using PyTorch's ```nn.Module```. Design the input layer to match the number of features in the MNIST dataset and the output layer to have as many neurons as there are classes (10). You can experiment with the number of hidden layers and neurons to optimize the performance. **Bonus scores**: *Make your architecture flexibile to have as many hidden layers as the user wants, and use hyperparameter optimization to select the best number of hidden layeres.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([2, 784])\n"
          ]
        }
      ],
      "source": [
        "print(features[0:2].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# class MNISTNeuralNetwork(nn.Module):\n",
        "\n",
        "#   def __init__(self, input_layer_size = 28*28, output_layer_size = 10):\n",
        "#     super().__init__()\n",
        "    \n",
        "#     self.first_layer = nn.Linear(input_layer_size,256)\n",
        "#     self.act1 = nn.ReLU()\n",
        "#     self.second_layer = nn.Linear(256, 128)\n",
        "#     self.act2 = nn.ReLU()\n",
        "\n",
        "#     self.third_layer = nn.Linear(128, 64)\n",
        "#     self.act3 = nn.ReLU()\n",
        "\n",
        "#     self.output = nn.Linear(64, output_layer_size)\n",
        "#     #self.output_act = nn.Softmax(dim=1) #we will use CrossEntropyLoss so we don't need softmax, because CE includes softmax\n",
        "\n",
        "#   def forward(self, x):\n",
        "#     x = self.act1(self.first_layer(x))\n",
        "#     x = self.act2(self.second_layer(x))\n",
        "#     x = self.act2(self.third_layer(x))\n",
        "#     x = self.output(x)\n",
        "#     return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MNISTNeuralNetwork(nn.Module):\n",
        "\n",
        "  def __init__(self, input_layer_size = 28*28, hidden_layers = [256, 128, 64], output_layer_size = 10):\n",
        "    super().__init__()\n",
        "\n",
        "    self.layers = nn.ModuleList()\n",
        "\n",
        "    # Add the first layer\n",
        "    self.layers.append(nn.Linear(input_layer_size, hidden_layers[0]))\n",
        "    self.layers.append(nn.ReLU())\n",
        "\n",
        "    # Add the hidden layers\n",
        "    for i in range(len(hidden_layers) - 1):\n",
        "      self.layers.append(nn.Linear(hidden_layers[i], hidden_layers[i+1]))\n",
        "      self.layers.append(nn.ReLU())\n",
        "\n",
        "    # Add the output layer\n",
        "    self.layers.append(nn.Linear(hidden_layers[-1], output_layer_size))\n",
        "\n",
        "  def forward(self, x):\n",
        "    for layer in self.layers:\n",
        "      x = layer(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3. **Loss Function and Optimizer**: Choose an appropriate loss function for multiclass classification. Select an optimizer, like SGD (Stochastic Gradient Descent) or Adam."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = MNISTNeuralNetwork()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "MNISTNeuralNetwork(\n",
              "  (layers): ModuleList(\n",
              "    (0): Linear(in_features=784, out_features=256, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Linear(in_features=256, out_features=128, bias=True)\n",
              "    (3): ReLU()\n",
              "    (4): Linear(in_features=128, out_features=64, bias=True)\n",
              "    (5): ReLU()\n",
              "    (6): Linear(in_features=64, out_features=10, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "loss_fn = nn.CrossEntropyLoss()  # cross entropy\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4. **Training**: Write a training loop to iterate over the dataset.\n",
        "Forward pass the input through the network, calculate the loss, and perform backpropagation. Update the weights of the network using the chosen optimizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(model, epochs, train_loader, loss_fn, optimizer):\n",
        "    model.train()  # Set the model to training mode\n",
        "    total_loss = 0\n",
        "    for epoch in range(epochs):\n",
        "        for batch in train_loader:\n",
        "            # Assuming your batch is a tuple (data, targets)\n",
        "            data, targets = batch\n",
        "\n",
        "            # Forward pass\n",
        "            output = model(data)\n",
        "\n",
        "            # Compute the loss\n",
        "            loss = loss_fn(output, targets)\n",
        "\n",
        "            # Backward pass and optimize\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "        print(f'Finished epoch {epoch + 1}, latest loss {loss}')\n",
        "    # Return average loss\n",
        "    return total_loss / len(train_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Finished epoch 1, latest loss 0.18443210422992706\n",
            "Finished epoch 2, latest loss 0.1391804814338684\n",
            "Finished epoch 3, latest loss 0.1525273621082306\n",
            "Finished epoch 4, latest loss 0.017943955957889557\n",
            "Finished epoch 5, latest loss 0.018885096535086632\n",
            "Finished epoch 6, latest loss 0.11280129104852676\n",
            "Finished epoch 7, latest loss 0.04102528840303421\n",
            "Finished epoch 8, latest loss 0.02543696202337742\n",
            "Finished epoch 9, latest loss 0.030377862975001335\n",
            "Finished epoch 10, latest loss 0.0013342633610591292\n",
            "The average loss is:  0.8162300818092771\n"
          ]
        }
      ],
      "source": [
        "average_loss = train(model, 10, train_loader, loss_fn, optimizer)\n",
        "\n",
        "print(\"The average loss is: \", average_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "5. **Testing**: Evaluate the trained model on the test set. Calculate the accuracy of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict(model, dataloader, num_predictions=10):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    all_predictions = []\n",
        "    all_actuals = []\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient computations\n",
        "        for i, batch in enumerate(dataloader):\n",
        "            # Assuming your batch is a tuple (data, targets)\n",
        "            data, targets = batch\n",
        "\n",
        "            output = model(data)\n",
        "            _, predicted = torch.max(output, dim=1)  # Get the predicted class\n",
        "\n",
        "            all_predictions.extend(predicted.tolist())\n",
        "            all_actuals.extend(targets.tolist())\n",
        "\n",
        "            # Break the loop after num_predictions batches\n",
        "            if i >= num_predictions - 1:\n",
        "                break\n",
        "\n",
        "    return all_predictions[:num_predictions], all_actuals[:num_predictions]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predictions: [6, 6, 9, 9, 0, 2, 0, 5, 0]\n",
            "Actuals: [6, 6, 9, 9, 0, 2, 0, 5, 0]\n"
          ]
        }
      ],
      "source": [
        "predictions, actuals = predict(model, test_loader, 10)\n",
        "print('Predictions:', predictions[:9])\n",
        "print('Actuals:', actuals[:9])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "6. **Optimization**: Experiment with hyperparameters (learning rate, number of epochs, etc.) to optimize the model's performance. Consider adjusting the neural network architecture for better results. **Notice that you can't use the optimization algorithms from scikit-learn that we saw in lab1: e.g.,** ```GridSearchCV```."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I've tried multiple number of epochs and batch sizes. For a greater number of epochs than 0, the algorithm already starts to overfit, so 10 is a good number. The same thing applies for the number of batch sizes.\n",
        "I tried with a bigger learning rate, but the training wouldn't be consistent, it won't find the minimum loss everytime."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
